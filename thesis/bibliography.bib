
@article{tan_efficientnet_2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2021-12-15},
	journal = {arXiv:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv: 1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	urldate = {2021-12-15},
	journal = {arXiv:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv: 1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	urldate = {2021-12-15},
	journal = {arXiv:1612.03144 [cs]},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = apr,
	year = {2017},
	note = {arXiv: 1612.03144},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2021-12-15},
	journal = {arXiv:1610.02357 [cs]},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv: 1610.02357},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{xie_aggregated_2017,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.05431},
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
	urldate = {2021-12-15},
	journal = {arXiv:1611.05431 [cs]},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.05431},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2021-12-15},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2021-12-15},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv: 1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	urldate = {2021-12-15},
	journal = {arXiv:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv: 1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-12-15},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-12-15},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2021-12-15},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wu_group_2018,
	title = {Group {Normalization}},
	url = {http://arxiv.org/abs/1803.08494},
	abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
	urldate = {2021-12-15},
	journal = {arXiv:1803.08494 [cs]},
	author = {Wu, Yuxin and He, Kaiming},
	month = jun,
	year = {2018},
	note = {arXiv: 1803.08494},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {http://arxiv.org/abs/1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2021-12-15},
	journal = {arXiv:1710.09412 [cs, stat]},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = apr,
	year = {2018},
	note = {arXiv: 1710.09412},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhou_unet_2018,
	title = {{UNet}++: {A} {Nested} {U}-{Net} {Architecture} for {Medical} {Image} {Segmentation}},
	shorttitle = {{UNet}++},
	url = {http://arxiv.org/abs/1807.10165},
	abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
	urldate = {2021-12-15},
	journal = {arXiv:1807.10165 [cs, eess, stat]},
	author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.10165},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-12-15},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{futrega_optimized_2021,
	title = {Optimized {U}-{Net} for {Brain} {Tumor} {Segmentation}},
	url = {http://arxiv.org/abs/2110.03352},
	abstract = {We propose an optimized U-Net architecture for a brain {\textbackslash}mbox\{tumor\} segmentation task in the BraTS21 Challenge. To find the {\textbackslash}mbox\{optimal\} model architecture and learning schedule we ran an extensive ablation study to test: deep supervision loss, Focal loss, decoder attention, drop block, and residual connections. Additionally, we have searched for the optimal depth of the U-Net and number of convolutional channels. Our solution was the winner of the challenge validation phase, with the normalized statistical ranking score of 0.267 and mean Dice score of 0.8855},
	urldate = {2021-12-15},
	journal = {arXiv:2110.03352 [cs, eess]},
	author = {Futrega, Michał and Milesi, Alexandre and Marcinkiewicz, Michal and Ribalta, Pablo},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.03352},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2021-12-15},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shelhamer_fully_2016,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1605.06211},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	urldate = {2021-12-15},
	journal = {arXiv:1605.06211 [cs]},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06211},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{xie_self-training_2020,
	title = {Self-training with {Noisy} {Student} improves {ImageNet} classification},
	url = {http://arxiv.org/abs/1911.04252},
	abstract = {We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.},
	urldate = {2021-12-15},
	journal = {arXiv:1911.04252 [cs, stat]},
	author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
	month = jun,
	year = {2020},
	note = {arXiv: 1911.04252},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pham_meta_2021,
	title = {Meta {Pseudo} {Labels}},
	url = {http://arxiv.org/abs/2003.10580},
	abstract = {We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2\% on ImageNet, which is 1.6\% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at https://github.com/google-research/google-research/tree/master/meta\_pseudo\_labels.},
	urldate = {2021-12-15},
	journal = {arXiv:2003.10580 [cs, stat]},
	author = {Pham, Hieu and Dai, Zihang and Xie, Qizhe and Luong, Minh-Thang and Le, Quoc V.},
	month = mar,
	year = {2021},
	note = {arXiv: 2003.10580},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{klinghoffer_self-supervised_2020,
	title = {Self-{Supervised} {Feature} {Extraction} for {3D} {Axon} {Segmentation}},
	url = {http://arxiv.org/abs/2004.09629},
	abstract = {Existing learning-based methods to automatically trace axons in 3D brain imagery often rely on manually annotated segmentation labels. Labeling is a labor-intensive process and is not scalable to whole-brain analysis, which is needed for improved understanding of brain function. We propose a self-supervised auxiliary task that utilizes the tube-like structure of axons to build a feature extractor from unlabeled data. The proposed auxiliary task constrains a 3D convolutional neural network (CNN) to predict the order of permuted slices in an input 3D volume. By solving this task, the 3D CNN is able to learn features without ground-truth labels that are useful for downstream segmentation with the 3D U-Net model. To the best of our knowledge, our model is the first to perform automated segmentation of axons imaged at subcellular resolution with the SHIELD technique. We demonstrate improved segmentation performance over the 3D U-Net model on both the SHIELD PVGPe dataset and the BigNeuron Project, single neuron Janelia dataset.},
	urldate = {2021-12-15},
	journal = {arXiv:2004.09629 [cs, eess]},
	author = {Klinghoffer, Tzofi and Morales, Peter and Park, Young-Gyun and Evans, Nicholas and Chung, Kwanghun and Brattain, Laura J.},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.09629},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{zbontar_barlow_2021,
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	shorttitle = {Barlow {Twins}},
	url = {http://arxiv.org/abs/2103.03230},
	abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
	urldate = {2021-12-15},
	journal = {arXiv:2103.03230 [cs, q-bio]},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	month = jun,
	year = {2021},
	note = {arXiv: 2103.03230},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
}

@article{grill_bootstrap_2020,
	title = {Bootstrap your own latent: {A} new approach to self-supervised {Learning}},
	shorttitle = {Bootstrap your own latent},
	url = {http://arxiv.org/abs/2006.07733},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3{\textbackslash}\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6{\textbackslash}\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
	urldate = {2021-12-15},
	journal = {arXiv:2006.07733 [cs, stat]},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	month = sep,
	year = {2020},
	note = {arXiv: 2006.07733},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2021-12-15},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.05709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhuang_self-supervised_2019,
	title = {Self-supervised {Feature} {Learning} for {3D} {Medical} {Images} by {Playing} a {Rubik}'s {Cube}},
	url = {http://arxiv.org/abs/1910.02241},
	abstract = {Witnessed the development of deep learning, increasing number of studies try to build computer aided diagnosis systems for 3D volumetric medical data. However, as the annotations of 3D medical data are difficult to acquire, the number of annotated 3D medical images is often not enough to well train the deep learning networks. The self-supervised learning deeply exploiting the information of raw data is one of the potential solutions to loose the requirement of training data. In this paper, we propose a self-supervised learning framework for the volumetric medical images. A novel proxy task, i.e., Rubik's cube recovery, is formulated to pre-train 3D neural networks. The proxy task involves two operations, i.e., cube rearrangement and cube rotation, which enforce networks to learn translational and rotational invariant features from raw 3D data. Compared to the train-from-scratch strategy, fine-tuning from the pre-trained network leads to a better accuracy on various tasks, e.g., brain hemorrhage classification and brain tumor segmentation. We show that our self-supervised learning approach can substantially boost the accuracies of 3D deep learning networks on the volumetric medical datasets without using extra data. To our best knowledge, this is the first work focusing on the self-supervised learning of 3D neural networks.},
	urldate = {2021-12-15},
	journal = {arXiv:1910.02241 [cs, eess]},
	author = {Zhuang, Xinrui and Li, Yuexiang and Hu, Yifan and Ma, Kai and Yang, Yujiu and Zheng, Yefeng},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.02241},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{tang_self-supervised_2022,
	title = {Self-{Supervised} {Pre}-{Training} of {Swin} {Transformers} for {3D} {Medical} {Image} {Analysis}},
	url = {http://arxiv.org/abs/2111.14791},
	abstract = {Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pre-training; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art (i.e. ranked 1st) on the public test leaderboards of both MSD and BTCV datasets. Code: https://monai.io/research/swin-unetr},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Tang, Yucheng and Yang, Dong and Li, Wenqi and Roth, Holger and Landman, Bennett and Xu, Daguang and Nath, Vishwesh and Hatamizadeh, Ali},
	month = mar,
	year = {2022},
	note = {arXiv:2111.14791 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{po-yao_masked_2022,
	title = {Masked {Autoencoders} that {Listen}},
	url = {http://arxiv.org/abs/2207.06405},
	abstract = {This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Po-Yao and Huang and Xu, Hu and Li, Juncheng and Baevski, Alexei and Auli, Michael and Galuba, Wojciech and Metze, Florian and Feichtenhofer, Christoph},
	month = jul,
	year = {2022},
	note = {arXiv:2207.06405 [cs, eess]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Technical report},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/LIPXEL2F/Po-Yao et al. - 2022 - Masked Autoencoders that Listen.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/LYRPTYTK/2207.html:text/html},
}

@misc{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = mar,
	year = {2022},
	note = {arXiv:2009.06732 [cs]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{borgeaud_improving_2022,
	title = {Improving language models by retrieving from trillions of tokens},
	url = {http://arxiv.org/abs/2112.04426},
	abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\${\textbackslash}times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
	month = feb,
	year = {2022},
	note = {Number: arXiv:2112.04426
arXiv:2112.04426 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Fix incorrect reported numbers in Table 14},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/FNR4SDN7/Borgeaud et al. - 2022 - Improving language models by retrieving from trill.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/RWMMBBMA/2112.html:text/html},
}

@techreport{mehta_separable_2022,
	title = {Separable {Self}-attention for {Mobile} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2206.02680},
	abstract = {Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires \$O(k{\textasciicircum}2)\$ time complexity with respect to the number of tokens (or patches) \$k\$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. \$O(k)\$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6\% on the ImageNet dataset, outperforming MobileViT by about 1\% while running \$3.2{\textbackslash}times\$ faster on a mobile device. Our source code is available at: {\textbackslash}url\{https://github.com/apple/ml-cvnets\}},
	number = {arXiv:2206.02680},
	urldate = {2022-06-13},
	institution = {arXiv},
	author = {Mehta, Sachin and Rastegari, Mohammad},
	month = jun,
	year = {2022},
	note = {arXiv:2206.02680 [cs]
version: 1
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@inproceedings{mauch_omras2_2009,
	title = {{OMRAS2} metadata project 2009},
	abstract = {Shared data enable researchers to reliably compare their results with others. The Centre for Digital Music (C4DM) has a tradition of providing ground truth data for research, and our chord, onset and segmentation annotations [2, 1, 3] have been used by many researchers in the MIR community. This year, we have focused on extending these resources in scope and quantity. 1 Ground Truth Annotations Ground truth in MIR refers to metadata in the form of annotations produced and/or checked by human beings. This ground truth, for example beat locations in a pop song, can then be used to evaluate a feature extraction algorithm, or for machine learning. We publish four different kinds of ground truth data on a range of popular music, manually annotated at the C4DM. In particular, in order to support recent research integrating the analysis of harmony (chord and key) and rhythmic structure (beat, bar and section), we have annotated these features for a collection of 179 Beatles songs. The annotations exist as plain text files and RDF files. Beat and metric position. These files contain beat positions in physical time as well as the metric position in a bar of each beat (”1 ” is a bar boundary), and contains pieces by the Beatles (179 pieces) and Zweieck (18). Chord. These files contain the annotations of chord onsets in physical time using Harte’s chord syntax [2]. Contains pieces by Carole King (14), Queen (20), and Zweieck (18) as well as the previously released 180 Beatles transcriptions. An additional MIDI-like version of the chords in an XML allows playback of the chords along with the original audio in Sonic Visualiser 1. Key. This metadata consists of the onset of musical keys in physical time. The collection contains annotations of pieces by the Beatles (179), Carole King (14), Queen (20) and Zweieck (18). Segmentation. These text files describe the onset in physical time of song structure elements (such as chorus and verse), and contains pieces by the},
	booktitle = {In {Late}-breaking session at the 10th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR}},
	author = {Mauch, M. and Cannam, C. and Davies, M. and Dixon, S. and Harte, C. and Kolozali, S. and Tidhar, D.},
	year = {2009},
	file = {Citeseer - Full Text PDF:/home/jan/Zotero/storage/LLMV8XND/Mauch et al. - 2009 - OMRAS2 metadata project 2009.pdf:application/pdf},
}

@misc{noauthor_jams_2022,
	title = {jams},
	copyright = {ISC},
	url = {https://github.com/marl/jams},
	abstract = {A JSON Annotated Music Specification for Reproducible MIR Research},
	urldate = {2022-06-11},
	publisher = {Music and Audio Research Laboratory - NYU},
	month = may,
	year = {2022},
	note = {original-date: 2014-10-29T09:07:42Z},
}

@article{goto_rwc_nodate,
	title = {{RWC} {Music} {Database}: {Popular}, {Classical}, and {Jazz} {Music} {Databases}},
	abstract = {This paper describes the design policy and speciﬁcations of the RWC Music Database, a music database (DB) that is available to researchers for common use and research purposes. Various commonly available DBs have been built in other research ﬁelds and have made a signiﬁcant contribution to the research in those ﬁelds. The ﬁeld of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Music Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI ﬁles, and text ﬁles of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.},
	language = {en},
	author = {Goto, Masataka},
	pages = {2},
	file = {Goto - RWC Music Database Popular, Classical, and Jazz M.pdf:/home/jan/Zotero/storage/PLH7H79P/Goto - RWC Music Database Popular, Classical, and Jazz M.pdf:application/pdf},
}

@article{berenzweig_large-scale_2004,
	title = {A {Large}-{Scale} {Evaluation} of {Acoustic} and {Subjective} {Music}-{Similarity} {Measures}},
	volume = {28},
	issn = {0148-9267, 1531-5169},
	url = {https://direct.mit.edu/comj/article/28/2/63-76/93900},
	doi = {10.1162/014892604323112257},
	language = {en},
	number = {2},
	urldate = {2022-06-11},
	journal = {Computer Music Journal},
	author = {Berenzweig, Adam and Logan, Beth and Ellis, Daniel P.W. and Whitman, Brian},
	month = jun,
	year = {2004},
	pages = {63--76},
	file = {Berenzweig et al. - 2004 - A Large-Scale Evaluation of Acoustic and Subjectiv.pdf:/home/jan/Zotero/storage/KXLAM6NU/Berenzweig et al. - 2004 - A Large-Scale Evaluation of Acoustic and Subjectiv.pdf:application/pdf},
}

@article{burgoyne_expert_2011,
	title = {{AN} {EXPERT} {GROUND}-{TRUTH} {SET} {FOR} {AUDIO} {CHORD} {RECOGNITION} {AND} {MUSIC} {ANALYSIS}},
	abstract = {Audio chord recognition has attracted much interest in recent years, but a severe lack of reliable training data—both in terms of quantity and range of sampling—has hindered progress. Working with a team of trained jazz musicians, we have collected time-aligned transcriptions of the harmony in more than a thousand songs selected randomly from the Billboard “Hot 100” chart in the United States between 1958 and 1991. These transcriptions contain complete information about upper extensions and alterations as well as information about meter, phrase, and larger musical structure. We expect that these transcriptions will enable signiﬁcant advances in the quality of training for audio-chord-recognition algorithms, and furthermore, because of an innovative sampling methodology, the data are usable as they stand for computational musicology. The paper includes some summary ﬁgures and statistics to help readers understand the scope of the data as well as information for obtaining the transcriptions for their own research.},
	language = {en},
	journal = {Oral Session},
	author = {Burgoyne, John Ashley and Wild, Jonathan and Fujinaga, Ichiro},
	year = {2011},
	pages = {6},
	file = {Burgoyne et al. - 2011 - AN EXPERT GROUND-TRUTH SET FOR AUDIO CHORD RECOGNI.pdf:/home/jan/Zotero/storage/M67CV3NV/Burgoyne et al. - 2011 - AN EXPERT GROUND-TRUTH SET FOR AUDIO CHORD RECOGNI.pdf:application/pdf},
}

@article{harte_towards_nodate,
	title = {Towards {Automatic} {Extraction} of {Harmony} {Information} from {Music} {Signals}},
	language = {en},
	author = {Harte, Christopher},
	pages = {283},
	file = {Harte - Towards Automatic Extraction of Harmony Informatio.pdf:/home/jan/Zotero/storage/99G58XBH/Harte - Towards Automatic Extraction of Harmony Informatio.pdf:application/pdf},
}

@article{de_clercq_corpus_2011,
	title = {A corpus analysis of rock harmony},
	volume = {30},
	issn = {0261-1430, 1474-0095},
	url = {https://www.cambridge.org/core/product/identifier/S026114301000067X/type/journal_article},
	doi = {10.1017/S026114301000067X},
	abstract = {Abstract
            
              In this study, we report a corpus analysis of rock harmony. As a corpus, we used
              Rolling Stone
              magazine's list of the ‘500 Greatest Songs of All Time’; we took the 20 top-ranked songs from each decade (the 1950s through the 1990s), creating a set of 100 songs. Both authors analysed all 100 songs by hand, using conventional Roman numeral symbols. Agreement between the two sets of analyses was over 90 per cent. The analyses were encoded using a recursive notation, similar to a context-free grammar, allowing repeating sections to be encoded succinctly. The aggregate data was then subjected to a variety of statistical analyses. We examined the frequency of different chords and chord transitions. The results showed that IV is the most common chord after I and is especially common preceding the tonic. Other results concern the frequency of different root motions, patterns of co-occurrence between chords, and changes in harmonic practice across time.},
	language = {en},
	number = {1},
	urldate = {2022-06-11},
	journal = {Popular Music},
	author = {de Clercq, Trevor and Temperley, David},
	month = jan,
	year = {2011},
	pages = {47--70},
	file = {de Clercq i Temperley - 2011 - A corpus analysis of rock harmony.pdf:/home/jan/Zotero/storage/4ZWMTVI5/de Clercq i Temperley - 2011 - A corpus analysis of rock harmony.pdf:application/pdf},
}

@techreport{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {http://arxiv.org/abs/2201.03545},
	abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
	number = {arXiv:2201.03545},
	urldate = {2022-06-07},
	institution = {arXiv},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	month = mar,
	year = {2022},
	note = {arXiv:2201.03545 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@techreport{kim_squeezeformer_2022,
	title = {Squeezeformer: {An} {Efficient} {Transformer} for {Automatic} {Speech} {Recognition}},
	shorttitle = {Squeezeformer},
	url = {http://arxiv.org/abs/2206.00888},
	abstract = {The recently proposed Conformer model has become the de facto backbone model for various downstream speech tasks based on its hybrid attention-convolution architecture that captures both local and global features. However, through a series of systematic studies, we find that the Conformer architecture's design choices are not optimal. After reexamining the design choices for both the macro and micro-architecture of Conformer, we propose the Squeezeformer model, which consistently outperforms the state-of-the-art ASR models under the same training schemes. In particular, for the macro-architecture, Squeezeformer incorporates (i) the Temporal U-Net structure, which reduces the cost of the multi-head attention modules on long sequences, and (ii) a simpler block structure of feed-forward module, followed up by multi-head attention or convolution modules, instead of the Macaron structure proposed in Conformer. Furthermore, for the micro-architecture, Squeezeformer (i) simplifies the activations in the convolutional block, (ii) removes redundant Layer Normalization operations, and (iii) incorporates an efficient depth-wise downsampling layer to efficiently sub-sample the input signal. Squeezeformer achieves state-of-the-art results of 7.5\%, 6.5\%, and 6.0\% word-error-rate on Librispeech test-other without external language models. This is 3.1\%, 1.4\%, and 0.6\% better than Conformer-CTC with the same number of FLOPs. Our code is open-sourced and available online.},
	number = {arXiv:2206.00888},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Kim, Sehoon and Gholami, Amir and Shaw, Albert and Lee, Nicholas and Mangalam, Karttikeya and Malik, Jitendra and Mahoney, Michael W. and Keutzer, Kurt},
	month = jun,
	year = {2022},
	note = {arXiv:2206.00888 [cs, eess]
version: 1
type: article},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/CACKNMDZ/Kim et al. - 2022 - Squeezeformer An Efficient Transformer for Automa.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/GET66NNH/2206.html:text/html},
}

@misc{noauthor_aicrowd_nodate,
	title = {{AIcrowd} {\textbar} {Music} {Demixing} {Challenge} {ISMIR} 2021 {\textbar} {Challenges}},
	url = {https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021},
	language = {en},
	urldate = {2022-05-27},
	journal = {AIcrowd {\textbar} Music Demixing Challenge ISMIR 2021 {\textbar} Challenges},
}

@misc{noauthor_dcase_nodate,
	title = {{DCASE}},
	url = {https://dcase.community/},
	urldate = {2022-05-27},
}

@misc{piczak_karolpiczakesc-50_2022,
	title = {karolpiczak/{ESC}-50},
	url = {https://github.com/karolpiczak/ESC-50},
	abstract = {ESC-50: Dataset for Environmental Sound Classification},
	urldate = {2022-05-27},
	author = {Piczak, Karol J.},
	month = may,
	year = {2022},
	note = {original-date: 2015-04-15T09:14:50Z},
	keywords = {audio, dataset, environmental-sounds},
}

@techreport{gong_ast_2021,
	title = {{AST}: {Audio} {Spectrogram} {Transformer}},
	shorttitle = {{AST}},
	url = {http://arxiv.org/abs/2104.01778},
	abstract = {In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6\% accuracy on ESC-50, and 98.1\% accuracy on Speech Commands V2.},
	number = {arXiv:2104.01778},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Gong, Yuan and Chung, Yu-An and Glass, James},
	month = jul,
	year = {2021},
	note = {arXiv:2104.01778 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound},
}

@techreport{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	number = {arXiv:2006.11477},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv:2006.11477 [cs, eess]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
}

@techreport{stoller_wave-u-net_2018,
	title = {Wave-{U}-{Net}: {A} {Multi}-{Scale} {Neural} {Network} for {End}-to-{End} {Audio} {Source} {Separation}},
	shorttitle = {Wave-{U}-{Net}},
	url = {http://arxiv.org/abs/1806.03185},
	abstract = {Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.},
	number = {arXiv:1806.03185},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
	month = jun,
	year = {2018},
	note = {arXiv:1806.03185 [cs, eess, stat]
type: article},
	keywords = {Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@techreport{defossez_hybrid_2021,
	title = {Hybrid {Spectrogram} and {Waveform} {Source} {Separation}},
	url = {http://arxiv.org/abs/2111.03600},
	abstract = {Source separation models either work on the spectrogram or waveform domain. In this work, we show how to perform end-to-end hybrid source separation, letting the model decide which domain is best suited for each source, and even combining both. The proposed hybrid version of the Demucs architecture won the Music Demixing Challenge 2021 organized by Sony. This architecture also comes with additional improvements, such as compressed residual branches, local attention or singular value regularization. Overall, a 1.4 dB improvement of the Signal-To-Distortion (SDR) was observed across all sources as measured on the MusDB HQ dataset, an improvement confirmed by human subjective evaluation, with an overall quality rated at 2.83 out of 5 (2.36 for the non hybrid Demucs), and absence of contamination at 3.04 (against 2.37 for the non hybrid Demucs and 2.44 for the second ranking model submitted at the competition).},
	number = {arXiv:2111.03600},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Défossez, Alexandre},
	month = nov,
	year = {2021},
	note = {arXiv:2111.03600 [cs, eess, stat]
type: article},
	keywords = {Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@techreport{rabe_self-attention_2021,
	title = {Self-attention {Does} {Not} {Need} \${O}(n{\textasciicircum}2)\$ {Memory}},
	url = {http://arxiv.org/abs/2112.05682},
	abstract = {We present a very simple algorithm for attention that requires \$O(1)\$ memory with respect to sequence length and an extension to self-attention that requires \$O({\textbackslash}log n)\$ memory. This is in contrast with the frequently stated belief that self-attention requires \$O(n{\textasciicircum}2)\$ memory. While the time complexity is still \$O(n{\textasciicircum}2)\$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires \$O({\textbackslash}sqrt\{n\})\$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.},
	number = {arXiv:2112.05682},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Rabe, Markus N. and Staats, Charles},
	month = dec,
	year = {2021},
	note = {arXiv:2112.05682 [cs]
version: 2
type: article},
	keywords = {Computer Science - Machine Learning},
}

@techreport{chen_vision_2022,
	title = {Vision {Transformer} {Adapter} for {Dense} {Predictions}},
	url = {http://arxiv.org/abs/2205.08534},
	abstract = {This work investigates a simple yet powerful adapter for Vision Transformer (ViT). Unlike recent visual transformers that introduce vision-specific inductive biases into their architectures, ViT achieves inferior performance on dense prediction tasks due to lacking prior information of images. To solve this issue, we propose a Vision Transformer Adapter (ViT-Adapter), which can remedy the defects of ViT and achieve comparable performance to vision-specific models by introducing inductive biases via an additional architecture. Specifically, the backbone in our framework is a vanilla transformer that can be pre-trained with multi-modal data. When fine-tuning on downstream tasks, a modality-specific adapter is used to introduce the data and tasks' prior information into the model, making it suitable for these tasks. We verify the effectiveness of our ViT-Adapter on multiple downstream tasks, including object detection, instance segmentation, and semantic segmentation. Notably, when using HTC++, our ViT-Adapter-L yields 60.1 box AP and 52.1 mask AP on COCO test-dev, surpassing Swin-L by 1.4 box AP and 1.0 mask AP. For semantic segmentation, our ViT-Adapter-L establishes a new state-of-the-art of 60.5 mIoU on ADE20K val, 0.6 points higher than SwinV2-G. We hope that the proposed ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. The code and models will be released at https://github.com/czczup/ViT-Adapter.},
	number = {arXiv:2205.08534},
	urldate = {2022-05-23},
	institution = {arXiv},
	author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
	month = may,
	year = {2022},
	note = {arXiv:2205.08534 [cs]
version: 2
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bittner_lightweight_2022,
	title = {A {Lightweight} {Instrument}-{Agnostic} {Model} for {Polyphonic} {Note} {Transcription} and {Multipitch} {Estimation}},
	url = {http://arxiv.org/abs/2203.09893},
	abstract = {Automatic Music Transcription (AMT) has been recognized as a key enabling technology with a wide range of applications. Given the task's complexity, best results have typically been reported for systems focusing on specific settings, e.g. instrument-specific systems tend to yield improved results over instrument-agnostic methods. Similarly, higher accuracy can be obtained when only estimating frame-wise \$f\_0\$ values and neglecting the harder note event detection. Despite their high accuracy, such specialized systems often cannot be deployed in the real-world. Storage and network constraints prohibit the use of multiple specialized models, while memory and run-time constraints limit their complexity. In this paper, we propose a lightweight neural network for musical instrument transcription, which supports polyphonic outputs and generalizes to a wide variety of instruments (including vocals). Our model is trained to jointly predict frame-wise onsets, multipitch and note activations, and we experimentally show that this multi-output structure improves the resulting frame-level note accuracy. Despite its simplicity, benchmark results show our system's note estimation to be substantially better than a comparable baseline, and its frame-level accuracy to be only marginally below those of specialized state-of-the-art AMT systems. With this work we hope to encourage the community to further investigate low-resource, instrument-agnostic AMT systems.},
	language = {en},
	urldate = {2022-05-16},
	publisher = {arXiv},
	author = {Bittner, Rachel M. and Bosch, Juan José and Rubinstein, David and Meseguer-Brocal, Gabriel and Ewert, Sebastian},
	month = may,
	year = {2022},
	note = {Number: arXiv:2203.09893
arXiv:2203.09893 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{cj_autochord_2022,
	title = {autochord},
	copyright = {Apache-2.0},
	url = {https://github.com/cjbayron/autochord},
	abstract = {Automatic Chord Recognition tools},
	urldate = {2022-05-14},
	author = {CJ},
	month = may,
	year = {2022},
	note = {original-date: 2021-04-20T19:34:30Z},
	keywords = {chord-estimation, chord-recognition, deep-learning, machine-learning, mir, music-information-retrieval},
}

@misc{ohollo_chord-extractor_2022,
	title = {chord-extractor},
	copyright = {GPL-2.0},
	url = {https://github.com/ohollo/chord-extractor},
	abstract = {Python library for extracting chords from multiple sound file formats},
	urldate = {2022-05-14},
	author = {ohollo},
	month = may,
	year = {2022},
	note = {original-date: 2021-01-27T21:17:11Z},
}

@misc{shah_guitar-chords-recognition_2022,
	title = {Guitar-{Chords}-recognition},
	copyright = {GPL-3.0},
	url = {https://github.com/ayushkumarshah/Guitar-Chords-recognition},
	abstract = {An application that predicts the chords when melspectrograms of guitar sound is fed into a CNN.},
	urldate = {2022-05-14},
	author = {Shah, Ayush Kumar},
	month = may,
	year = {2022},
	note = {original-date: 2019-02-08T02:37:30Z},
}

@article{bayron_autochord_nodate,
	title = {{AUTOCHORD}: {AUTOMATIC} {CHORD} {RECOGNITION} {LIBRARY} {AND} {CHORD} {VISUALIZATION} {APP}},
	abstract = {In this paper I present autochord, a bundle of tools for automatic chord recognition, comprising of 1) a Python library that performs Audio Chord Estimation (ACE), and 2) a JavaScript app for visualizing and comparing chord labels, all open-source and freely available online.1 The Python library (hereinafter referred to as autochord.py) can generate MIREX-style chord labels 2 which can be interpreted and visualized by the app (hereinafter referred to as autochord.js). Used together, this toolset functions as a full chord recognition app.},
	language = {en},
	author = {Bayron, Christopher John},
	pages = {3},
	file = {Bayron - AUTOCHORD AUTOMATIC CHORD RECOGNITION LIBRARY AND.pdf:/home/jan/Zotero/storage/LXIGZNWF/Bayron - AUTOCHORD AUTOMATIC CHORD RECOGNITION LIBRARY AND.pdf:application/pdf},
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2022-05-12},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-05-10},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@article{tarvainen_mean_2018,
	title = {Mean teachers are better role models: {Weight}-averaged consistency targets improve semi-supervised deep learning results},
	shorttitle = {Mean teachers are better role models},
	url = {http://arxiv.org/abs/1703.01780},
	abstract = {The recently proposed Temporal Ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, Temporal Ensembling becomes unwieldy when learning large datasets. To overcome this problem, we propose Mean Teacher, a method that averages model weights instead of label predictions. As an additional beneﬁt, Mean Teacher improves test accuracy and enables training with fewer labels than Temporal Ensembling. Without changing the network architecture, Mean Teacher achieves an error rate of 4.35\% on SVHN with 250 labels, outperforming Temporal Ensembling trained with 1000 labels. We also show that a good network architecture is crucial to performance. Combining Mean Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with 4000 labels from 10.55\% to 6.28\%, and on ImageNet 2012 with 10\% of the labels from 35.24\% to 9.11\%.},
	language = {en},
	urldate = {2022-05-10},
	journal = {arXiv:1703.01780 [cs, stat]},
	author = {Tarvainen, Antti and Valpola, Harri},
	month = apr,
	year = {2018},
	note = {arXiv: 1703.01780},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{wang_solving_2021,
	title = {Solving {Inefficiency} of {Self}-supervised {Representation} {Learning}},
	url = {http://arxiv.org/abs/2104.08760},
	abstract = {Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model's superiority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. Codes available at: https://github.com/wanggrun/triplet .},
	language = {en},
	urldate = {2022-05-10},
	journal = {arXiv:2104.08760 [cs]},
	author = {Wang, Guangrun and Wang, Keze and Wang, Guangcong and Torr, Philip H. S. and Lin, Liang},
	month = oct,
	year = {2021},
	note = {arXiv: 2104.08760},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{wortsman_model_2022,
	title = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
	shorttitle = {Model soups},
	url = {http://arxiv.org/abs/2203.05482},
	abstract = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of ﬁne-tuning large pre-trained models, where ﬁne-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models ﬁne-tuned with diﬀerent hyperparameter conﬁgurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs—we call the results “model soups.” When ﬁne-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides signiﬁcant improvements over the best model in a hyperparameter sweep on ImageNet. As a highlight, the resulting ViT-G model attains 90.94\% top-1 accuracy on ImageNet, a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classiﬁcation and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to ﬂatness of the loss and conﬁdence of the predictions, and validate this relation empirically.},
	language = {en},
	urldate = {2022-05-10},
	journal = {arXiv:2203.05482 [cs]},
	author = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S. and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and Schmidt, Ludwig},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.05482},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@article{beyer_better_2022,
	title = {Better plain {ViT} baselines for {ImageNet}-1k},
	url = {http://arxiv.org/abs/2205.01580},
	abstract = {It is commonly accepted that the Vision Transformer model requires sophisticated regularization techniques to excel at ImageNet-1k scale data. Surprisingly, we ﬁnd this is not the case and standard data augmentation is sufﬁcient. This note presents a few minor modiﬁcations to the original Vision Transformer (ViT) vanilla training setting that dramatically improve the performance of plain ViT models. Notably, 90 epochs of training surpass 76\% top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic ResNet50 baseline, and 300 epochs of training reach 80\% in less than one day.},
	language = {en},
	urldate = {2022-05-10},
	journal = {arXiv:2205.01580 [cs]},
	author = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},
	month = may,
	year = {2022},
	note = {arXiv: 2205.01580},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{tu_maxvit_2022,
	title = {{MaxViT}: {Multi}-{Axis} {Vision} {Transformer}},
	shorttitle = {{MaxViT}},
	url = {http://arxiv.org/abs/2204.01697},
	abstract = {Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to "see" globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5{\textbackslash}\% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7{\textbackslash}\% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. We will make the code and models publicly available.},
	urldate = {2022-05-10},
	journal = {arXiv:2204.01697 [cs]},
	author = {Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.01697
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{el-nouby_xcit_2021,
	title = {{XCiT}: {Cross}-{Covariance} {Image} {Transformers}},
	shorttitle = {{XCiT}},
	url = {http://arxiv.org/abs/2106.09681},
	abstract = {Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a "transposed" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.},
	urldate = {2022-05-10},
	journal = {arXiv:2106.09681 [cs]},
	author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Hervé},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09681
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{noauthor_chordify_2022,
	title = {Chordify {Annotator} {Subjectivity} {Dataset}},
	url = {https://github.com/chordify/CASD},
	abstract = {Chordify Annotator Subjectivity Dataset - A chord-Label harmony dataset with multiple reference annotations per song},
	urldate = {2022-05-07},
	publisher = {Chordify},
	month = feb,
	year = {2022},
	note = {original-date: 2017-10-16T12:45:25Z},
	keywords = {dataset, music-information-retrieval, annotator-subjectivity},
}

@misc{admin_about_nodate,
	title = {About},
	url = {https://chordify.net/pages/about/},
	abstract = {Find out all there is to know about Chordify, our technology and our team. This page also provides you with our contact information.},
	language = {en-US},
	urldate = {2022-05-07},
	journal = {Blog {\textbar} Chordify {\textbar} Tune Into Chords},
	author = {admin},
}

@misc{noauthor_instant_nodate,
	title = {Instant chords for any song - {Chordify}},
	url = {https://chordify.net/},
	urldate = {2022-05-07},
}

@misc{noauthor_madmom_2022,
	title = {madmom},
	url = {https://github.com/CPJKU/madmom},
	abstract = {Python audio and music signal processing library},
	urldate = {2022-05-04},
	publisher = {Institute of Computational Perception},
	month = may,
	year = {2022},
	note = {original-date: 2015-09-08T08:19:06Z},
	keywords = {machine-learning, music-information-retrieval, audio-analysis, cython, numpy, python, scipy, signal-processing},
}

@misc{noauthor_librosa_2022,
	title = {librosa},
	copyright = {ISC},
	url = {https://github.com/librosa/librosa},
	abstract = {Python library for audio and music analysis},
	urldate = {2022-05-04},
	publisher = {librosa},
	month = may,
	year = {2022},
	note = {original-date: 2012-10-20T14:21:01Z},
	keywords = {audio, python, scipy, dsp, librosa, music, nyucds},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2022-05-04},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{humphrey_rethinking_2012,
	title = {Rethinking {Automatic} {Chord} {Recognition} with {Convolutional} {Neural} {Networks}},
	volume = {2},
	doi = {10.1109/ICMLA.2012.220},
	abstract = {Despite early success in automatic chord recognition, recent efforts are yielding diminishing returns while basically iterating over the same fundamental approach. Here, we abandon typical conventions and adopt a different perspective of the problem, where several seconds of pitch spectra are classified directly by a convolutional neural network. Using labeled data to train the system in a supervised manner, we achieve state of the art performance through this initial effort in an otherwise unexplored area. Subsequent error analysis provides insight into potential areas of improvement, and this approach to chord recognition shows promise for future harmonic analysis systems.},
	booktitle = {2012 11th {International} {Conference} on {Machine} {Learning} and {Applications}},
	author = {Humphrey, Eric J. and Bello, Juan P.},
	month = dec,
	year = {2012},
	keywords = {Accuracy, automatic music transcription, chord recognition, Computer architecture, convolutional neural nets, Kernel, Neural networks, Training, Training data, Vectors},
	pages = {357--362},
	annote = {Pierwszy pomysł wykorzystania splotowych sieci neuronowych (i w ogóle deep learningu) do rozpoznawania akordów. Model end-to-end, czyli wszystkie etapy rozpoznawania akordów zawarte w jednej, klasycznej sieci splotowej.
},
	file = {Rethinking_Automatic_Chord_Recognition_with_Convolutional_Neural_Networks.pdf:/home/jan/Zotero/storage/QQVQ2JSK/Rethinking_Automatic_Chord_Recognition_with_Convolutional_Neural_Networks.pdf:application/pdf},
}

@inproceedings{humphrey_music_2014,
	title = {From music audio to chord tablature: {Teaching} deep convolutional networks toplay guitar},
	shorttitle = {From music audio to chord tablature},
	doi = {10.1109/ICASSP.2014.6854952},
	abstract = {Automatic chord recognition is conventionally tackled as a general music audition task, where the desired output is a time-aligned sequence of discrete chord symbols, e.g. CMaj7, Esus2, etc. In practice, however, this presents two related challenges: one, the act of decoding a given chord sequence requires that the musician knows both the notes in the chord and how to play them on some instrument; and two, chord labeling systems do not degrade gracefully for users without significant musical training. Alternatively, we address both challenges by modeling the physical constraints of a guitar to produce human-readable representations of music audio, i.e guitar tablature via a deep convolutional network. Through training and evaluation as a standard chord recognition system, the model is able to yield representations that require minimal prior knowledge to interpret, while maintaining respectable performance compared to the state of the art.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Humphrey, Eric J. and Bello, Juan P.},
	month = may,
	year = {2014},
	note = {ISSN: 2379-190X},
	keywords = {chord recognition, Training, Data models, deep networks, guitar tablature, Hidden Markov models, Multiple signal classification, Music, representation learning, Shape, Vocabulary},
	pages = {6974--6978},
	file = {From_music_audio_to_chord_tablature_Teaching_deep_convolutional_networks_toplay_guitar.pdf:/home/jan/Zotero/storage/D5NNRTSG/From_music_audio_to_chord_tablature_Teaching_deep_convolutional_networks_toplay_guitar.pdf:application/pdf},
}

@article{korzeniowski_feature_2016,
	title = {Feature {Learning} for {Chord} {Recognition}: {The} {Deep} {Chroma} {Extractor}},
	shorttitle = {Feature {Learning} for {Chord} {Recognition}},
	url = {http://arxiv.org/abs/1612.05065},
	abstract = {We explore frame-level audio feature learning for chord recognition using artificial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artificial neural networks. It is trained to compute chroma features that encode harmonic information important for chord recognition, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and resolve harmonic ambiguities. We compare the resulting features to hand-crafted ones by using a simple linear frame-wise classifier for chord recognition on various data sets. The results show that the learned feature extractor produces superior chroma vectors for chord recognition.},
	urldate = {2022-03-10},
	journal = {arXiv:1612.05065 [cs]},
	author = {Korzeniowski, Filip and Widmer, Gerhard},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.05065},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {Wydaje się, że pierwsza z całej serii prac Korzeniowskiego nad tym tematem. Cała koncepcja opiera się na założeniach, że z trzech etapów rozpoznawnia akordów (ekstrakcja cech, klasyfikacja i detekcja sekwencji) najważniejszy i kluczowy jest pierwszy etap a pozstałe nie mają takiego dużego znaczenia. Argumentują to wynikami dla linowego klasyfikatora na chromagramie z GT. Pomysł polega więc na wytrenowaniu MLP, który dostaje spektrogram (z STFT i filtrów logarytmicznych) z 1.5s kontekstem i tworzy chromagram (zamiast tworzyć go ręcznie jak to było wcześniej). Taka sieć ma automatycznie wybierać to co ważne i usuwać szum. Faktycznie tak wyuczony ekstraktor cech pozwala uzyskać (regresją liniową) znacznie lepsze wyniki niż inne, klasyczne cechy. Dodatkowo autorzy analizują działanie warstw ukrytych i dochodzą na co sieć właściwie zwraca uwagę - w kontekście czasu i częstotliwości) - nic czego nie można by się spodziewać, po prostu taki lepszy PCP.
},
	file = {1612.05065.pdf:/home/jan/Zotero/storage/5EC4SX4T/1612.05065.pdf:application/pdf},
}

@inproceedings{humphrey_learning_2012,
	title = {Learning a robust {Tonnetz}-space transform for automatic chord recognition},
	doi = {10.1109/ICASSP.2012.6287914},
	abstract = {Temporal pitch class profiles - commonly referred to as a chromagrams - are the de facto standard signal representation for content-based methods of musical harmonic analysis, despite exhibiting a set of practical difficulties. Here, we present a novel, data-driven approach to learning a robust function that projects audio data into Tonnetz-space, a geometric representation of equal-tempered pitch intervals grounded in music theory. We apply this representation to automatic chord recognition and show that our approach out-performs the classification accuracy of previous chroma representations, while providing a mid-level feature space that circumvents challenges inherent to chroma.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Humphrey, Eric J. and Cho, Taemin and Bello, Juan P.},
	month = mar,
	year = {2012},
	note = {ISSN: 2379-190X},
	keywords = {Accuracy, Kernel, Training, Chord Recognition, Convolutional Neural Networks, Deep Learning, Harmonic analysis, Time frequency analysis, Tonnetz, Transforms, USA Councils},
	pages = {453--456},
	file = {Learning_a_robust_Tonnetz-space_transform_for_automatic_chord_recognition.pdf:/home/jan/Zotero/storage/5DHM6NDL/Learning_a_robust_Tonnetz-space_transform_for_automatic_chord_recognition.pdf:application/pdf},
}

@inproceedings{korzeniowski_fully_2016,
	title = {A fully convolutional deep auditory model for musical chord recognition},
	doi = {10.1109/MLSP.2016.7738895},
	abstract = {Chord recognition systems depend on robust feature extraction pipelines. While these pipelines are traditionally hand-crafted, recent advances in end-to-end machine learning have begun to inspire researchers to explore data-driven methods for such tasks. In this paper, we present a chord recognition system that uses a fully convolutional deep auditory model for feature extraction. The extracted features are processed by a Conditional Random Field that decodes the final chord sequence. Both processing stages are trained automatically and do not require expert knowledge for optimising parameters. We show that the learned auditory system extracts musically interpretable features, and that the proposed chord recognition system achieves results on par or better than state-of-the-art algorithms.},
	booktitle = {2016 {IEEE} 26th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	author = {Korzeniowski, Filip and Widmer, Gerhard},
	month = sep,
	year = {2016},
	keywords = {chord recognition, Neural networks, Training, Hidden Markov models, Computational modeling, conditional random fields, Convolution, convolutional neural networks, Decoding, Feature extraction},
	pages = {1--6},
	annote = {
Pomysł polega na wykorzystaniu w pełni splotowej sieci neuronowej (na wzór VGG z nowszymi rozwiązaniami jak GAP i dropout) do ekstrakcji cech a następnie nałożenie na to CRF do detekcji (w tym wygładzania) sekwencji akordów. Opiera się o poprzednią pracę z serii czyli “deep chroma extractor”. Sieć splotowa i CRF uczone osobno chociaż stanowią w zasadzie jedną całość. Działa dobrze jak inne SOT w tym czasie.
},
	file = {A_fully_convolutional_deep_auditory_model_for_musical_chord_recognition.pdf:/home/jan/Zotero/storage/LNUW92EH/A_fully_convolutional_deep_auditory_model_for_musical_chord_recognition.pdf:application/pdf},
}

@inproceedings{yang_highlighting_2016,
	title = {Highlighting root notes in chord recognition using cepstral features and multi-task learning},
	doi = {10.1109/APSIPA.2016.7820865},
	abstract = {A musical chord is usually described by its root note and the chord type. While a substantial amount of work has been done in the field of music information retrieval (MIR) to automate chord recognition, the role of root notes in this task has seldom received specific attention. In this paper, we present a new approach and empirical studies demonstrating improved accuracy in chord recognition by properly highlighting the information of the root notes. In the signal level, we propose to combine spectral features with features derived from the cepstrum to improve the identification of low pitches, which usually correspond to the root notes. In the model level, we propose a multi-task learning framework based on the neural nets to jointly consider chord recognition and root note recognition in training. We found that the improved accuracy can be attributed to better information about the sub-harmonics of the notes, and the emphasis of root notes in recognizing chords.},
	booktitle = {2016 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA})},
	author = {Yang, Mu-Heng and Su, Li and Yang, Yi-Hsuan},
	month = dec,
	year = {2016},
	keywords = {Training, Music, Harmonic analysis, Cepstrum, Mel frequency cepstral coefficient, Time-frequency analysis},
	pages = {1--8},
	file = {Highlighting_root_notes_in_chord_recognition_using_cepstral_features_and_multi-task_learning.pdf:/home/jan/Zotero/storage/ACET73PY/Highlighting_root_notes_in_chord_recognition_using_cepstral_features_and_multi-task_learning.pdf:application/pdf},
}

@inproceedings{sigtia_audio_2015,
	title = {Audio {Chord} {Recognition} with a {Hybrid} {Recurrent} {Neural} {Network}},
	abstract = {The ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic signal, eliminating the need for a complex feature extraction stage. In this paper, we present a novel architecture for audio chord estimation using a hybrid recurrent neural network. The architecture replaces hidden Markov models (HMMs) with recurrent neural network (RNN) based language models for modelling temporal dependencies between chords. We demonstrate the ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic signal, eliminating the need for a complex feature extraction stage. For the hybrid RNN architecture, inference over the output variables of interest is performed using beam search. In addition to the hybrid model, we propose a modification to beam search using a hash table which yields improved results while reducing memory requirements by an order of magnitude, thus making the proposed model suitable for real-time applications. We evaluate our model's performance on a dataset with publicly available annotations and demonstrate that the performance is comparable to existing state of the art approaches for chord recognition.},
	booktitle = {{ISMIR}},
	author = {Sigtia, Siddharth and Boulanger-Lewandowski, Nicolas and Dixon, S.},
	year = {2015},
	file = {000227.pdf:/home/jan/Zotero/storage/T472GGTA/000227.pdf:application/pdf},
}

@inproceedings{boulanger-lewandowski_audio_2013,
	title = {Audio {Chord} {Recognition} with {Recurrent} {Neural} {Networks}},
	abstract = {An efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account is devised and the resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task. In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task.},
	booktitle = {{ISMIR}},
	author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
	year = {2013},
	file = {000243.pdf:/home/jan/Zotero/storage/AN5349IX/000243.pdf:application/pdf},
}

@inproceedings{kamonsantiroj_improving_2017,
	title = {Improving {Pitch} {Class} {Profile} for {Musical} {Chords} {Recognition} {Combining} {Major} {Chord} {Filters} and {Convolution} {Neural} {Networks}},
	doi = {10.1109/IIAI-AAI.2017.37},
	abstract = {In this paper, we consider the challenging problem of music recognition and present an effective deep learning based method using a convolution neural network for chord recognition. It has known that a pitch class profile (PCP) is the commonly signal representation of musical harmonic analysis. However, the PCP vector is not expressive enough for chord recognition, which often occurs in many real-world environments. In this study, we extend the PCP vector scheme to address the limitation. Our proposed method basically consists of two major steps. First, we introduce novel filters and apply then to PCP vector to transform the vector into membership of 7 major chords as features to represent the input matrix. The second step is to efficiently learning feature on the transformed matrix (2D-PCP) using convolution neural network. We propose a trainable, data-driven approach that automatically learns features and its classifier simultaneously. Experimental results conducted on the task of musical chords recognition that the proposed method achieves improvements of classification accuracy more than 40\% in accuracy in comparing with based line methods.},
	booktitle = {2017 6th {IIAI} {International} {Congress} on {Advanced} {Applied} {Informatics} ({IIAI}-{AAI})},
	author = {Kamonsantiroj, Suwatchai and Wannatrong, Lita and Pipanmaekaporn, Luepol},
	month = jul,
	year = {2017},
	keywords = {Neural networks, Music, Harmonic analysis, Convolution, Feature extraction, Audio Chord Recognition, C\# languages, Convolution Neural Network, Feature Learning, Pitch Class Profile, Power harmonic filters},
	pages = {880--885},
	file = {Improving_Pitch_Class_Profile_for_Musical_Chords_Recognition_Combining_Major_Chord_Filters_and_Convolution_Neural_Networks.pdf:/home/jan/Zotero/storage/YX4J8LWP/Improving_Pitch_Class_Profile_for_Musical_Chords_Recognition_Combining_Major_Chord_Filters_and_Convolution_Neural_Networks.pdf:application/pdf},
}

@inproceedings{hori_music_2017,
	title = {Music chord recognition from audio data using bidirectional encoder-decoder {LSTMs}},
	doi = {10.1109/APSIPA.2017.8282235},
	abstract = {In this paper, we discuss some methods for chord recognition based on long short-term memory recurrent neural networks (LSTM, LSTM-RNN). Chord progressions play an important role in the generation process of music. Actually, music processing systems containing a model for chord progressions achieve high accuracies in tasks like music structure analysis, multi pitch analysis an automatic composition or accompaniment. In previous research, chord progressions were obtained rule- based or have been modeled using stochastic methods like hidden Markov models or probabilistic context-free grammars. Pitch patterns were then regarded as the observations resulting from the hidden states of the chord progression model. Recently, con- volutional neural networks have been used for chord recognition with considerable success. On the other hand, LSTM networks have been shown to be suitable for generating chord progressions, since these neural networks can process time series data very well. The purpose of this study is to evaluate and compare three types of LSTM networks based on the bidirectional and encoderdecoder structure with regards to their chord recognition performance. In order to extract more effective data for chord recognition, we use a constant-Q transform and specmurt analysis to suppress overtone components, and chroma vectorization to reduce the feature dimensionality. The evaluation results show that the encoder-decoder-based LSTM can learn the relationship between the observed chroma vectors and the associated chord progression more effectively than simpler LSTM networks.},
	booktitle = {2017 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Hori, Takeshi and Nakamura, Kazuyuki and Sagayama, Shigeki},
	month = dec,
	year = {2017},
	keywords = {Neural networks, Hidden Markov models, Music, Harmonic analysis, Databases, Instruments, Spectrogram},
	pages = {1312--1315},
	file = {Music_chord_recognition_from_audio_data_using_bidirectional_encoder-decoder_LSTMs.pdf:/home/jan/Zotero/storage/GCBL9DDE/Music_chord_recognition_from_audio_data_using_bidirectional_encoder-decoder_LSTMs.pdf:application/pdf},
}

@inproceedings{wu_music_2018,
	title = {Music {Chord} {Recognition} {Based} on {Midi}-{Trained} {Deep} {Feature} and {BLSTM}-{CRF} {Hybird} {Decoding}},
	doi = {10.1109/ICASSP.2018.8461439},
	abstract = {In this paper, we design a novel deep learning based hybrid system for automatic chord recognition. Currently, there is a bottleneck in the amount of enough annotated data for training robust acoustic models, as hand annotating time-synchronized chord labels requires professional musical skills and considerable labor. As a solution to this problem, we construct a large set of time synchronized MIDI-audio pairs, and use these data to train a Deep Residual Network (DRN) feature extractor, which can then estimate pitch class activations of real-world music audio recordings. Sequence classification and decoding are then performed with a trained Bidirectional LSTM and Conditional Random Fields (CRF) network. Experiments show that the proposed model is compatible for both regular major/minor triad chord classification and larger vocabulary chord recognition, the performance is good and no less than other state-of-the-art systems. The proposed system also achieved good evaluation score in MIREX 2017 Automatic Chord Estimation task.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Wu, Yiming and Li, Wei},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Automatic chord recognition, Bidirectional long short term memory (BLSTM), Conditional random fields (CRF), Deep residual network},
	pages = {376--380},
	file = {Music_Chord_Recognition_Based_on_Midi-Trained_Deep_Feature_and_BLSTM-CRF_Hybird_Decoding.pdf:/home/jan/Zotero/storage/AJ4NKGZZ/Music_Chord_Recognition_Based_on_Midi-Trained_Deep_Feature_and_BLSTM-CRF_Hybird_Decoding.pdf:application/pdf},
}

@inproceedings{korzeniowski_large-scale_2018,
	title = {A {Large}-{Scale} {Study} of {Language} {Models} for {Chord} {Prediction}},
	doi = {10.1109/ICASSP.2018.8462285},
	abstract = {We conduct a large-scale study of language models for chord prediction. Specifically, we compare N-gram models to various flavours of recurrent neural networks on a comprehensive dataset comprising all publicly available datasets of annotated chords known to us. This large amount of data allows us to systematically explore hyperparameter settings for the recurrent neural networks-a crucial step in achieving good results with this model class. Our results show not only a quantitative difference between the models, but also a qualitative one: in contrast to static N-gram models, certain RNN configurations adapt to the songs at test time. This finding constitutes a further step towards the development of chord recognition systems that are more aware of local musical context than what was previously possible.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Korzeniowski, Filip and Sears, David R. W. and Widmer, Gerhard},
	month = apr,
	year = {2018},
	note = {ISSN: 2379-190X},
	keywords = {Training, Data models, Hidden Markov models, Computational modeling, Adaptation models, Chord Prediction, Language Modelling, Predictive models, Recurrent neural networks, Recurrent Neural Networks},
	pages = {91--95},
	annote = {Czwarty artykuł z serii Korzeniowskiego. Stanowi uzupełnienie badań przeprowadzonych w poprzednim artykule. Badania te polegały na sprawdzeniu na dużą skalę jak sobie dadzą radę modele od NLP przy przewidywaniu sekwencji akordów - przy tworzeniu modelu języka akordów (chord language model). Chodziło tylko o same sekwencje akordów, bez rozpoznawania co to za akord z surowych danych. Dużo zbiorów danych było wziętych do tego. Porównane dwa rodzaje modeli: n-gram i sieci rekurencyjne. Oczywiście sieci rekurencyjne lepiej sobie poradziły i wskazali dla jakich hiperparametrów wyszło najlepiej. Niewiele z tego wynika poza tym, że sieci rekurencyjne są dużo lepsze od modelu n-gramów i że da się nimi przewidywać sekwencję akordów tak jak sekwencję słów w języku naturalnym.
},
	file = {A_Large-Scale_Study_of_Language_Models_for_Chord_Prediction.pdf:/home/jan/Zotero/storage/KWSLG8QT/A_Large-Scale_Study_of_Language_Models_for_Chord_Prediction.pdf:application/pdf},
}

@inproceedings{korzeniowski_automatic_2018,
	title = {Automatic {Chord} {Recognition} with {Higher}-{Order} {Harmonic} {Language} {Modelling}},
	doi = {10.23919/EUSIPCO.2018.8553600},
	abstract = {Common temporal models for automatic chord recognition model chord changes on a frame-wise basis. Due to this fact, they are unable to capture musical knowledge about chord progressions. In this paper, we propose a temporal model that enables explicit modelling of chord changes and durations. We then apply N -gram models and a neural-network-based acoustic model within this framework, and evaluate the effect of model overconfidence. Our results show that model overconfidence plays only a minor role (but target smoothing still improves the acoustic model), and that stronger chord language models do improve recognition results, however their effects are small compared to other domains.},
	booktitle = {2018 26th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Korzeniowski, Filip and Widnaer, Gerhard},
	month = sep,
	year = {2018},
	note = {ISSN: 2076-1465},
	keywords = {Training, Hidden Markov models, Chord Recognition, Computational modeling, Language Modelling, Predictive models, Acoustics, Europe, N-Grams, Neural Networks, Smoothing methods},
	pages = {1900--1904},
	file = {Automatic_Chord_Recognition_with_Higher-Order_Harmonic_Language_Modelling.pdf:/home/jan/Zotero/storage/RABKUUNU/Automatic_Chord_Recognition_with_Higher-Order_Harmonic_Language_Modelling.pdf:application/pdf},
}

@inproceedings{mcfee_structured_2017,
	title = {Structured {Training} for {Large}-{Vocabulary} {Chord} {Recognition}},
	abstract = {A deep convolutional-recurrent model for automatic chord recognition over a vocabulary of 170 classes is developed, trained to produce both the time-varying chord label sequence as well as binary encodings of chord roots and qualities. Automatic chord recognition systems operating in the large-vocabulary regime must overcome data scarcity: certain classes occur much less frequently than others, and this presents a significant challenge when estimating model parameters. While most systems model the chord recognition task as a (multi-class) classification problem, few attempts have been made to directly exploit the intrinsic structural similarities between chord classes. In this work, we develop a deep convolutional-recurrent model for automatic chord recognition over a vocabulary of 170 classes. To exploit structural relationships between chord classes, the model is trained to produce both the time-varying chord label sequence as well as binary encodings of chord roots and qualities. This binary encoding directly exposes similarities between related classes, allowing the model to learn a more coherent representation of simultaneous pitch content. Evaluations on a corpus of 1217 annotated recordings demonstrate substantial improvements compared to previous models.},
	booktitle = {{ISMIR}},
	author = {McFee, Brian and Bello, J.},
	year = {2017},
	file = {ismir2017_chord.pdf:/home/jan/Zotero/storage/VQF7JVXS/ismir2017_chord.pdf:application/pdf},
}

@article{deng_large_2018,
	title = {Large vocabulary automatic chord estimation using bidirectional long short-term memory recurrent neural network with even chance training},
	doi = {10.1080/09298215.2017.1367820},
	abstract = {Preliminary success is demonstrated in the proposed system framework with a skewed class-sensitive training scheme that leads to a preliminary solution to large vocabulary automatic chord estimation, and the even chance training scheme is proved to be effective in boosting uncommon chord symbol recalls as well as the average chord quality accuracy. Abstract This paper presents an argument for the necessity of a large vocabulary in automatic chord recognition systems, on the grounds of the requirements of machine musicianship. It proposes a system framework with a skewed class-sensitive training scheme that leads to a preliminary solution to large vocabulary automatic chord estimation. This framework applies a bidirectional long short-term memory recurrent neural network architecture, which employs an ‘even chance’ training scheme to make up for the lack of uncommon chords’ exposure. The main drawback of this approach is the low segmentation quality, which inevitably lowers the upper bound of chord estimation accuracy. Under a large vocabulary evaluation, the proposed system can significantly outperform the baseline system in terms of the overall weighted chord symbol recall, and there is no significant difference between them in terms of average chord quality accuracy. The results demonstrate preliminary success in our approach, and also prove the even chance training scheme to be effective in boosting uncommon chord symbol recalls as well as the average chord quality accuracy.},
	author = {Deng, Jun-qi and Kwok, Yu-Kwong},
	year = {2018},
}

@article{wu_automatic_2019,
	title = {Automatic {Audio} {Chord} {Recognition} {With} {MIDI}-{Trained} {Deep} {Feature} and {BLSTM}-{CRF} {Sequence} {Decoding} {Model}},
	volume = {27},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2018.2879399},
	abstract = {With the advances of machine learning technologies, data-driven feature extraction and sequence modeling approaches are being widely explored for automatic chord recognition tasks. Currently, there is a bottleneck in the amount of enough annotated data for training robust acoustic models, as hand-annotating time-synchronized chord labels requires professional musical skills and considerable labor. To cope with this limitation, in this paper, we propose a convolutional neural network (CNN) based deep feature extractor, which is trained on a large set of time, synchronized musical instrument digital interface audio data pairs and can robustly estimate pitch class activations of real-world music audio recordings. The CNN feature extractor plus a bidirectional long short-term memory conditional random field decoding model forms the proposed hybrid system for automatic chord recognition. Experiments show that the proposed model is compatible for both regular major/minor triad chord classification and larger vocabulary chord recognition, and outperforms other state-of-the-art chord recognition systems.},
	number = {2},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wu, Yiming and Li, Wei},
	month = feb,
	year = {2019},
	note = {Number: 2},
	keywords = {Training, Hidden Markov models, Music, Harmonic analysis, Decoding, Feature extraction, Automatic chord recognition, bidirectional long short-term memory (BLSTM), conditional random fields (CRF)},
	pages = {355--366},
	file = {Automatic_Audio_Chord_Recognition_With_MIDI-Trained_Deep_Feature_and_BLSTM-CRF_Sequence_Decoding_Model.pdf:/home/jan/Zotero/storage/3PYYZAR4/Automatic_Audio_Chord_Recognition_With_MIDI-Trained_Deep_Feature_and_BLSTM-CRF_Sequence_Decoding_Model.pdf:application/pdf},
}

@inproceedings{wu_automatic_2019-1,
	title = {Automatic {Chord} {Estimation} {Based} on a {Frame}-wise {Convolutional} {Recurrent} {Neural} {Network} with {Non}-{Aligned} {Annotations}},
	doi = {10.23919/EUSIPCO.2019.8902741},
	abstract = {This paper describes a weakly-supervised approach to Automatic Chord Estimation (ACE) task that aims to estimate a sequence of chords from a given music audio signal at the frame level, under a realistic condition that only non-aligned chord annotations are available. In conventional studies assuming the availability of time-aligned chord annotations, Deep Neural Networks (DNNs) that learn frame-wise mappings from acoustic features to chords have attained excellent performance. The major drawback of such frame-wise models is that they cannot be trained without the time alignment information. Inspired by a common approach in automatic speech recognition based on nonaligned speech transcriptions, we propose a two-step method that trains a Hidden Markov Model (HMM) for the forced alignment between chord annotations and music signals, and then trains a powerful frame-wise DNN model for ACE. Experimental results show that although the frame-level accuracy of the forced alignment was just under 90\%, the performance of the proposed method was degraded only slightly from that of the DNN model trained by using the ground-truth alignment data. Furthermore, using a sufficient amount of easily collected non-aligned data, the proposed method is able to reach or even outperform the conventional methods based on ground-truth time-aligned annotations.},
	booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Wu, Yiming and Carsault, Tristan and Yoshii, Kazuyoshi},
	month = sep,
	year = {2019},
	note = {ISSN: 2076-1465},
	keywords = {Training, Hidden Markov models, Multiple signal classification, Convolution, Feature extraction, and RNN, Annotations, Automatic chord estimation, CNN, Estimation, forced alignment, HMM},
	pages = {1--5},
}

@inproceedings{hanlon_fifthnet_2020,
	title = {The {Fifthnet} {Chroma} {Extractor}},
	doi = {10.1109/ICASSP40776.2020.9053714},
	abstract = {Deep Learning (DL) is commonly used in music processing tasks such as Automatic Chord Recognition (ACR), for which Convolutional Neural Networks (CNNs) are popular tools. Compression of CNNs has become a research topic of interest, focused on post-pruning of learnt networks and development of less expensive network elements. CNNs assemble high level structure in data from small simple patterns. Music signals are often processed in the spectral domain where much known structure is present. We propose the FifthNet, a neural network for chroma-based ACR that incorporates known spectral structures in its design through data manipulation. We find that FifthNet is competitive with popular ACR networks while using only a small fraction of their network parameters.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Hanlon, Ken O’ and Sandler, Mark B.},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {Neural networks, Multiple signal classification, Chord recognition, Chroma, Convolutional networks, Deep learning, DNN compression, Spectral analysis, Speech processing, Speech recognition, Task analysis, Tools},
	pages = {3752--3756},
	file = {The_Fifthnet_Chroma_Extractor.pdf:/home/jan/Zotero/storage/JE4KLDBK/The_Fifthnet_Chroma_Extractor.pdf:application/pdf},
}

@article{wu_semi-supervised_2020,
	title = {Semi-{Supervised} {Neural} {Chord} {Estimation} {Based} on a {Variational} {Autoencoder} {With} {Latent} {Chord} {Labels} and {Features}},
	volume = {28},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2020.3035001},
	abstract = {This paper describes a statistically-principled semi-supervised method of automatic chord estimation (ACE) that can make effective use of music signals regardless of the availability of chord annotations. The typical approach to ACE is to train a deep classification model (neural chord estimator) in a supervised manner by using only annotated music signals. In this discriminative approach, prior knowledge about chord label sequences (model output) has scarcely been taken into account. In contrast, we propose a unified generative and discriminative approach in the framework of amortized variational inference. More specifically, we formulate a deep generative model that represents the generative process of chroma vectors (observed variables) from discrete labels and continuous features (latent variables), which are assumed to follow a Markov model favoring self-transitions and a standard Gaussian distribution, respectively. Given chroma vectors as observed data, the posterior distributions of the latent labels and features are computed approximately by using deep classification and recognition models, respectively. These three models form a variational autoencoder and can be trained jointly in a semi-supervised manner. The experimental results show that the regularization of the classification model based on the Markov prior of chord labels and the generative model of chroma vectors improved the performance of ACE even under the supervised condition. The semi-supervised learning using additional non-annotated data can further improve the performance.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wu, Yiming and Carsault, Tristan and Nakamura, Eita and Yoshii, Kazuyoshi},
	year = {2020},
	keywords = {Hidden Markov models, Multiple signal classification, Music, Computational modeling, Automatic chord estimation, Estimation, Markov processes, semi-supervised learning, variational autoencoder},
	pages = {2956--2966},
	file = {Semi-Supervised_Neural_Chord_Estimation_Based_on_a_Variational_Autoencoder_With_Latent_Chord_Labels_and_Features.pdf:/home/jan/Zotero/storage/2UHHY2DE/Semi-Supervised_Neural_Chord_Estimation_Based_on_a_Variational_Autoencoder_With_Latent_Chord_Labels_and_Features.pdf:application/pdf},
}

@inproceedings{pauwels_20_2019,
	title = {20 {Years} of {Automatic} {Chord} {Recognition} from {Audio}},
	abstract = {Comunicacio presentada a: 20th annual conference of the International Society for Music Information Retrieval (ISMIR) celebrat del 4 al 8 de novembre de 2019 a Delft, Paisos Baixos. Comunicacio presentada a: 20th annual conference of the International Society for Music Information Retrieval (ISMIR) celebrat del 4 al 8 de novembre de 2019 a Delft, Paisos Baixos.},
	booktitle = {{ISMIR}},
	author = {Pauwels, J. and O'Hanlon, K. and Gómez, E. and Sandler, M.},
	year = {2019},
	file = {000004.pdf:/home/jan/Zotero/storage/4CIWX4R9/000004.pdf:application/pdf},
}

@inproceedings{chen_harmony_2019,
	title = {Harmony {Transformer}: {Incorporating} {Chord} {Segmentation} into {Harmony} {Recognition}},
	shorttitle = {Harmony {Transformer}},
	abstract = {The Harmony Transformer is proposed, a multi-task music harmony analysis model aiming to improve chord recognition through incorporating chord segmentation into the recognition process using end-to-end sequence learning. Musical harmony analysis is usually a process of unfolding and interpreting the hierarchical structure of music. Computational approaches to such structural analysis are still challenging, owing to the fact that the boundary between different harmonic states (such as chord functions) is not explicitly defined in the audio or symbolic music data. It is a novel approach to improve chord recognition by jointly identifying chord change using end-to-end sequence learning. In this paper, we propose the Harmony Transformer, a multi-task music harmony analysis model aiming to improve chord recognition through incorporating chord segmentation into the recognition process. The integration of chord segmentation and chord recognition is implemented with the Transformer, a deep sequential learning model yielding fruitful results in the field of natural language processing. A non-autoregressive decoding framework is also adopted here in aid of concatenating the two highly correlated tasks. Experiments of both chord symbol recognition and functional harmony recognition on audio and symbolic datasets demonstrate that explicitly learning the hierarchical structural information of musical data can facilitate and improve the harmony recognition.},
	booktitle = {{ISMIR}},
	author = {Chen, Tsung-Ping and Su, Li},
	year = {2019},
	annote = {Wzięli zwykły transformer i dodali do niego parę rzeczy: enkoder zwraca segmentację (gdzie się zmienia akord) a później bierze to dekoder i zwraca jakie to są akordy (klasyfikacja). W sumie dużo różnych tricków i prawdopodobnie nadmiarowych modyfikacji, jak branie wyjść z każdej warstwy enkodera, osobne lossy dla enkodera i dekodera (dla segmentacji i klasyfikacji osobno), jakiś dziwny embedding, itd...
Najgorsze jest to, że porównali to do jakiegoś własnego, trywialnego LSTMa i jednego kijowego modelu, podobnego (!) do modelu literaturowego i wyszło im, że wszystko zrobili zajebiście a nie porównali się właściwie do niczego sensownego. Nie zbadali też w żaden sposób czy te ich modyfikacje cokolwiek dają - nie wiadomo czy po prostu ich model nie jest większy i tyle. Wszystko sprawdzali na wyekstrachowanych wcześniej cechach - nie używali surowych danych.},
	file = {000030.pdf:/home/jan/Zotero/storage/MZPI966Q/000030.pdf:application/pdf},
}

@inproceedings{wu_variational_2020,
	title = {A {Variational} {Autoencoder} for {Joint} {Chord} and {Key} {Estimation} from {Audio} {Chromagrams}},
	abstract = {This paper describes a deep generative approach to jointly estimating chords and keys from music signals. Although deep neural networks have widely been used for estimating various kinds of musical elements, joint estimation of multiple kinds of musical elements has scarcely been investigated so far. Given the mutual dependency between keys and chords, which both describe the harmonic content of music, we propose to use a unified deep classification model for jointly estimating chords and keys. At the heart of our study is the integration of supervised multi-task learning with unsupervised variational autoencoding for achieving improved performance and semi-supervised learning. Specifically, we formulate a deep latent-variable model that represents the generative process of chroma vectors from discrete key classes, chord classes, and continuous latent features. The deep classification model and another deep recognition model are then introduced for inferring keys, chords, and latent features from chroma vectors. These three models are trained jointly in a (semi-)supervised manner, where the generative model acts as a regularizer for the classification model. The experimental results show that the multi-task learning improves the consistency between estimated keys and chords and that the autoencoding-based regularization significantly improves the estimation performance.},
	booktitle = {2020 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Wu, Yiming and Nakamura, Eita and Yoshii, Kazuyoshi},
	month = dec,
	year = {2020},
	note = {ISSN: 2640-0103},
	keywords = {Training, Training data, Data models, Hidden Markov models, Multiple signal classification, Music, Estimation},
	pages = {500--506},
	file = {A_Variational_Autoencoder_for_Joint_Chord_and_Key_Estimation_from_Audio_Chromagrams.pdf:/home/jan/Zotero/storage/CGTNDR4B/A_Variational_Autoencoder_for_Joint_Chord_and_Key_Estimation_from_Audio_Chromagrams.pdf:application/pdf},
}

@inproceedings{bortolozzo_improving_2021,
	title = {Improving the {Classification} of {Rare} {Chords} {With} {Unlabeled} {Data}},
	doi = {10.1109/ICASSP39728.2021.9413701},
	abstract = {In this work, we explore techniques to improve performance for rare classes in the task of Automatic Chord Recognition (ACR). We first explored the use of the focal loss in the context of ACR, which was originally proposed to improve the classification of hard samples. In parallel, we adapted a self-learning technique originally designed for image recognition to the musical domain. Our experiments show that both approaches individually (and their combination) improve the recognition of rare chords, but using only self-learning with noise addition yields the best results.},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Bortolozzo, Marcelo and Schramm, Rodrigo and Jung, Claudio R.},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Training data, Music, Speech recognition, Automatic Chord Recognition, Conferences, Data Imbalance, focal loss, Image recognition, Measurement, self-training, Signal processing},
	pages = {3390--3394},
	file = {Improving_the_Classification_of_Rare_Chords_With_Unlabeled_Data.pdf:/home/jan/Zotero/storage/3B7QI5HW/Improving_the_Classification_of_Rare_Chords_With_Unlabeled_Data.pdf:application/pdf},
}

@article{park_bi-directional_2019,
	title = {A {Bi}-directional {Transformer} for {Musical} {Chord} {Recognition}},
	url = {http://arxiv.org/abs/1907.02698},
	abstract = {Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model. In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance.},
	urldate = {2022-03-10},
	journal = {arXiv:1907.02698 [cs, eess]},
	author = {Park, Jonggwon and Choi, Kyoyun and Jeon, Sungwook and Kim, Dokyun and Park, Jonghun},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.02698},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {1907.02698.pdf:/home/jan/Zotero/storage/ULPV3LI9/1907.02698.pdf:application/pdf},
}

@inproceedings{ito_harmonic_2021,
	title = {Harmonic {Representation} for {CNN}-{LSTM} {Automatic} {Chord} {Recognition}},
	doi = {10.1109/ICORIS52787.2021.9649565},
	abstract = {Paper Since Chord progression is the element that determines the harmony of a piece of music, Automatic Chord Recognition (ACR) from audio signals is a crucial task in the field of Music Information Retrieval(MIR). Recently, various models using deep learning have been proposed, but there are few studies on their input features. Notes parts of the chord are the fundamental note, and its overtone ringed simultaneously. In order to model these audio signals efficiently, feature transforms such as “Constat-Q-Transform(CQT)” is used. However, due to the superposition of fundamental notes and overtones of various instruments in polyphonic music, it is considered difficult to model chords even by deep learning. Therefore, we focused on the structure, including fundamental notes are on the logarithm and its overtones are on the linear. In this paper, we propose a feature representation that can represent overtone structure for each fundamental note. Based on these feature representations, data-driven approach to learn the chord by CNN-LSTM model. We evaluated performance using 383 songs with publicly available annotations, and achieved the same performance with approximately one-tenth of the number of parameters than the existing methods.},
	booktitle = {2021 3rd {International} {Conference} on {Cybernetics} and {Intelligent} {System} ({ICORIS})},
	author = {Ito, Tsuyoshi and Arai, Shuichi},
	month = oct,
	year = {2021},
	keywords = {Multiple signal classification, Deep Learning, Harmonic analysis, Transforms, Instruments, Annotations, Deep learning, Automatic Chord Recognition, Music Information Retrieval, Pattern Recognition, Signal Processing, Target recognition},
	pages = {1--5},
	file = {Harmonic_Representation_for_CNN-LSTM_Automatic_Chord_Recognition.pdf:/home/jan/Zotero/storage/JKUDRIIX/Harmonic_Representation_for_CNN-LSTM_Automatic_Chord_Recognition.pdf:application/pdf},
}

@article{ohanlon_fifthnet_2021,
	title = {{FifthNet}: {Structured} {Compact} {Neural} {Networks} for {Automatic} {Chord} {Recognition}},
	volume = {29},
	issn = {2329-9304},
	shorttitle = {{FifthNet}},
	doi = {10.1109/TASLP.2021.3070158},
	abstract = {Deep learning has become popular for many music processing tasks with Convolutional Neural Networks (CNNs) often applied. CNNs can be computationally expensive, a problem that may be alleviated through design of compact network elements or by compressing trained networks. CNNs assemble high-level structure in a hierarchical fashion, starting from small simple local patterns. On the other hand, much structure found in music spectra, such as harmonicity, is already well-defined. Both signal representations and processing methods have previously exploited such structure. We propose FifthNet, a compact neural network that is applied to the task of Automatic Chord Recognition (ACR). The compactness of FifthNet is effected through exploiting known data structure; first by arranging the network inputs according to expected data structures, then by separating processing of the semantically meaningful dimensions of the data. FifthNet is then seen to perform similar to a state-of-the-art CNN for ACR while employing only a small percentage of the parameters and computational expense used by the CNN.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {O’Hanlon, Ken and Sandler, Mark B.},
	year = {2021},
	keywords = {Kernel, Neural networks, Multiple signal classification, Harmonic analysis, Convolution, Feature extraction, Deep learning, Task analysis, automatic chord recognition, chroma, compact network, network compression},
	pages = {2671--2682},
	file = {FifthNet_Structured_Compact_Neural_Networks_for_Automatic_Chord_Recognition(1).pdf:/home/jan/Zotero/storage/2EY54JSU/FifthNet_Structured_Compact_Neural_Networks_for_Automatic_Chord_Recognition(1).pdf:application/pdf},
}

@inproceedings{deng_automatic_2016,
	title = {Automatic {Chord} estimation on seventhsbass {Chord} vocabulary using deep neural network},
	doi = {10.1109/ICASSP.2016.7471677},
	abstract = {This paper proposes an automatic chord estimation (ACE) system with a two-layer architecture. The first layer performs chord smoothing with "GMM + HMM" approach. Then given the results of the first layer, the second layer performs chord estimation using a deep neural network, which is trained on a well chord-type balanced dataset. The system accepts exactly the "SeventhsBass" vocabulary. Three approaches with different configurations of the system are compared with Chordino, which is probably the only both MIREX evaluated and "SeventhsBass" acceptable ACE system. Evaluation results on "The Beatles" dataset show that the best approach outperforms Chordino in the most difficult "SeventhsBass" metric in a significant way.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Deng, Junqi and Kwok, Yu-Kwong},
	month = mar,
	year = {2016},
	note = {ISSN: 2379-190X},
	keywords = {Neural networks, Hidden Markov models, Vocabulary, Smoothing methods, Estimation, Measurement, Automatic Chord Estimation, Deep Belief Network, Deep Neural Network, Standards},
	pages = {261--265},
	file = {Automatic_Chord_estimation_on_seventhsbass_Chord_vocabulary_using_deep_neural_network.pdf:/home/jan/Zotero/storage/3VMQC5RJ/Automatic_Chord_estimation_on_seventhsbass_Chord_vocabulary_using_deep_neural_network.pdf:application/pdf},
}

@misc{noauthor_papers_2022,
	title = {Papers with {Code} - {AugmentedNet}: {A} {Roman} {Numeral} {Analysis} {Network} with {Synthetic} {Training} {Examples} and {Additional} {Tonal} {Tasks}},
	shorttitle = {Papers with {Code} - {AugmentedNet}},
	url = {https://paperswithcode.com/paper/augmentednet-a-roman-numeral-analysis-network},
	abstract = {Implemented in one code library.},
	language = {en},
	urldate = {2022-03-10},
	month = mar,
	year = {2022},
}

@article{korzeniowski_improved_2018,
	title = {Improved {Chord} {Recognition} by {Combining} {Duration} and {Harmonic} {Language} {Models}},
	url = {http://arxiv.org/abs/1808.05335},
	abstract = {Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model---to be applied on chord sequences---and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results.},
	urldate = {2022-03-10},
	journal = {arXiv:1808.05335 [cs, eess]},
	author = {Korzeniowski, Filip and Widmer, Gerhard},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.05335},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Artykuł ten stanowi kontynuację badań Korzeniowskiego z poprzednich artykułów na temat stosowania modeli sekwencyjnych (językowych) w ropoznawaniu akordów na poziomie sekwencji akordów a nie sekwencji ramek czasowych. W tym artykule opisana została metoda, jak w praktyce zastosować razem model akustyczny (rozpoznająćy akordy per ramka) i model językowy (rozpoznający sekwencję akordów), które to modele działają na różnych poziomach (na ramkach czasowych i na pojedynczych akordach). Autorzy definiują model propabilistyczny pozwalający zintegrować te dwa modele poprzez wprowadzenie modelu zmian akordów (duration model), który to przewiduje czy akord się zmieni w danej ramce czy nie, na podstawie ramek poprzednich.
Następnie autorzy podejmują się implementacji i przetestowania takiego zintegrowanego modelu. W tym celu wybierają jako model akustyczny model bardzo zbliżony do stosowanej już wcześniej sieci splotowej (w stylu VGG), również preprocessing pozostaje jak w poprzednich badaniach. Dwa pozostałe modele to sieci rekurencyjne, których hiperparametry i architketury są wyszukane w ramach eksperymentów. Wszystkie modele są trenowane osobno. 
Na końcu autorzy pokazują, że zintegrowany model ma potencjał - generalnie wprowadzenie tych modeli sekwnecyjnych usprawnia rozpoznawanie akordów. Pokazują różnice między różnymi rodzajami tych dwóch dodatkowych modeli. Nie ma niestety porównania do SOTA! Eksperymenty mają bardziej na celu zbadać działanie i skutki tych modeli językowych w zadaniu rozpoznawania akordów oraz pokazać ich potencjał, niż stworzyć nowy, lepszy model.
},
	file = {1808.05335.pdf:/home/jan/Zotero/storage/LJ7P32RW/1808.05335.pdf:application/pdf},
}

@inproceedings{mukherjee_segregating_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Segregating {Musical} {Chords} for {Automatic} {Music} {Transcription}: {A} {LSTM}-{RNN} {Approach}},
	isbn = {978-3-030-34872-4},
	shorttitle = {Segregating {Musical} {Chords} for {Automatic} {Music} {Transcription}},
	doi = {10.1007/978-3-030-34872-4_47},
	abstract = {Notating or transcribing a music piece is very important for musicians. It not only helps them to communicate among each other but also helps in understanding a piece. This is very much essential for improvisations and performances. This makes automatic music transcription systems extremely important. Every music piece can be broadly categorized into two parts namely the lead section and the accompaniment section or background music (BGM). The BGM is very important in a piece as it sets the mood and makes a piece complete. Thus it is very much important to notate the BGM for properly understanding and performing a piece. One of the key components of BGM is known as chord which is constituted of two or more musical notes. Every composition is accompanied with a chord chart. In this paper, a long short term memory-recurrent neural network (LSTM-RNN)- based approach is presented for segregating musical chords from clips of short durations which can aid in automatic transcription. Experiments were performed on over 46800 clips and a highest accuracy of 99.91\% has been obtained for the proposed system.},
	language = {en},
	booktitle = {Pattern {Recognition} and {Machine} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Mukherjee, Himadri and Dhar, Ankita and Obaidullah, Sk. Md. and Santosh, K. C. and Phadikar, Santanu and Roy, Kaushik},
	editor = {Deka, Bhabesh and Maji, Pradipta and Mitra, Sushmita and Bhattacharyya, Dhruba Kumar and Bora, Prabin Kumar and Pal, Sankar Kumar},
	year = {2019},
	keywords = {Chord identification, LSTM-RNN, Music signal},
	pages = {427--435},
	file = {Mukherjee2019_Chapter_SegregatingMusicalChordsForAut.pdf:/home/jan/Zotero/storage/Z8ZKWU32/Mukherjee2019_Chapter_SegregatingMusicalChordsForAut.pdf:application/pdf},
}

@inproceedings{humphrey_four_2015,
	title = {Four {Timely} {Insights} on {Automatic} {Chord} {Estimation}},
	abstract = {This work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations. Automatic chord estimation (ACE) is a hallmark research topic in content-based music informatics, but like many other tasks, system performance appears to be converging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite arguably achieving some of the highest results to date, both approaches plateau well short of having solved the problem. Therefore, this work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that invalidate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, standard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conventional approaches conflate the competing goals of recognition and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjective, making the very notion of “ground truth” annotations tenuous. Synthesizing these observations, this paper offers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large.},
	booktitle = {{ISMIR}},
	author = {Humphrey, Eric J. and Bello, J.},
	year = {2015},
	file = {000294.pdf:/home/jan/Zotero/storage/6C89JP79/000294.pdf:application/pdf},
}

@article{korzeniowski_futility_2017,
	title = {On the {Futility} of {Learning} {Complex} {Frame}-{Level} {Language} {Models} for {Chord} {Recognition}},
	url = {http://arxiv.org/abs/1702.00178},
	doi = {10.17743/aesconf.2017.978-1-942220-15-2},
	abstract = {Chord recognition systems use temporal models to post-process frame-wise chord preditions from acoustic models. Traditionally, first-order models such as Hidden Markov Models were used for this task, with recent works suggesting to apply Recurrent Neural Networks instead. Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is futile on principle, and that non-Markovian models do not perform better than their first-order counterparts. We support our argument through three experiments on the McGill Billboard dataset. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system. The third, still rather preliminary experiment gives first indications that the use of complex sequential models for chord prediction at higher temporal levels might be more promising.},
	urldate = {2022-03-10},
	journal = {2017 AES International Conference on Semantic Audio},
	author = {Korzeniowski, Filip and Widmer, Gerhard},
	year = {2017},
	note = {arXiv: 1702.00178},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {Trzecia z serii Korzeniowskiego. W pracy tej wykazana jest daremności w wykorzystywaniu złożonych modeli sekwencyjnych (językowych) jak sieci rekurencyjne na poziomie pojedynczych ramek czasowych do dekodowania sekwencji akordów. Na tym poziomie sieci te nie uzyskują przewagi nad prostszymi modelami jak HMM. Autorzy udowadniają natomiast, że sieci rekurencyjne mają bardzo duży potencjał na poziomie sekwnecji akordów. Autorzy dzielą model rozpoznający akordy na dwie części: akustyczną (acoustic) i czasową (temporal). Część akustyczna ropoznaje akordy dla pojedynczych ramek a czasowa dekoduje sekwnecję akordów z uwzględnieniem kontekstu czasowego. Przeprowadzają trzy eksperymenty aby to wszystko wykazać:
Pierwszy polega na porównaniu LSTM-RNN z HMM (first order). Modele są trenowane na sekwencjach akordów, próbkowanych z częstotliwością 10Hz. Sieć rekurencyjna sprawdziła się nieznacznie lepiej.
Drugi polega na zastosowaniu tych modeli w rozpoznawaniu akordów z prawdziwnych danych. Sprawdzone są wszystkie kombinacje dla trzech modeli akustycznych (regresja logistyczna, MLP i sieć splotowa) i czterech modeli czasowych (żaden, głosowanie większościowe, HMM i RNN) - HMM i RNN zdecydowanie poprawiają jakość ale między sobią się prawie nie różnią, HMM daje nieznacznie lepsze wyniki. Poza tym sieć splotowa daje najlepsze wyniki.
Trzeci eksperyment to udowodnienie, że sieć rekurencyjna jest znacznie lepsza i ma potencjał, ale na poziomie sekwencji akordów a nie pojedynczych ramek czasowych. Autorzy powtarzają pierwszy eksperyment dla całych akordów i wychodzi że RNN jest znacznie lepszy. Pokazują nawet wykresy, na których widać zdolność RNN do rozpoznawania i przewidywania cykli akordów w utworze.
},
	file = {1702.00178.pdf:/home/jan/Zotero/storage/BNLZPJA2/1702.00178.pdf:application/pdf},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-04-28},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
}

@article{xie_pgl_2020,
	title = {{PGL}: {Prior}-{Guided} {Local} {Self}-supervised {Learning} for {3D} {Medical} {Image} {Segmentation}},
	shorttitle = {{PGL}},
	url = {http://arxiv.org/abs/2011.12640},
	abstract = {It has been widely recognized that the success of deep learning in image segmentation relies overwhelmingly on a myriad amount of densely annotated training data, which, however, are difficult to obtain due to the tremendous labor and expertise required, particularly for annotating 3D medical images. Although self-supervised learning (SSL) has shown great potential to address this issue, most SSL approaches focus only on image-level global consistency, but ignore the local consistency which plays a pivotal role in capturing structural information for dense prediction tasks such as segmentation. In this paper, we propose a PriorGuided Local (PGL) self-supervised model that learns the region-wise local consistency in the latent feature space. Specifically, we use the spatial transformations, which produce different augmented views of the same image, as a prior to deduce the location relation between two views, which is then used to align the feature maps of the same local region but being extracted on two views. Next, we construct a local consistency loss to minimize the voxel-wise discrepancy between the aligned feature maps. Thus, our PGL model learns the distinctive representations of local regions, and hence is able to retain structural information. This ability is conducive to downstream segmentation tasks. We conducted an extensive evaluation on four public computerized tomography (CT) datasets that cover 11 kinds of major human organs and two tumors. The results indicate that using pre-trained PGL model to initialize a downstream network leads to a substantial performance improvement over both random initialization and the initialization with global consistency-based models. Code and pre-trained weights will be made available at: https://git.io/PGL.},
	urldate = {2022-03-30},
	journal = {arXiv:2011.12640 [cs]},
	author = {Xie, Yutong and Zhang, Jianpeng and Liao, Zehui and Xia, Yong and Shen, Chunhua},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.12640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{caron_unsupervised_2021,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	url = {http://arxiv.org/abs/2006.09882},
	abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
	urldate = {2022-03-25},
	journal = {arXiv:2006.09882 [cs]},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	month = jan,
	year = {2021},
	note = {arXiv: 2006.09882},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{goyal_vision_2022,
	title = {Vision {Models} {Are} {More} {Robust} {And} {Fair} {When} {Pretrained} {On} {Uncurated} {Images} {Without} {Supervision}},
	url = {http://arxiv.org/abs/2202.08360},
	abstract = {Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to object centric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, robustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on object centric datasets such as ImageNet.},
	urldate = {2022-03-25},
	journal = {arXiv:2202.08360 [cs]},
	author = {Goyal, Priya and Duval, Quentin and Seessel, Isaac and Caron, Mathilde and Misra, Ishan and Sagun, Levent and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.08360},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@article{chen_big_2020,
	title = {Big {Self}-{Supervised} {Models} are {Strong} {Semi}-{Supervised} {Learners}},
	url = {http://arxiv.org/abs/2006.10029},
	abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (\${\textbackslash}le\$13 labeled images per class) using ResNet-50, a \$10{\textbackslash}times\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
	urldate = {2022-03-25},
	journal = {arXiv:2006.10029 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.10029},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{el-nouby_xcit_2021-1,
	title = {{XCiT}: {Cross}-{Covariance} {Image} {Transformers}},
	shorttitle = {{XCiT}},
	url = {http://arxiv.org/abs/2106.09681},
	abstract = {Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a "transposed" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.},
	urldate = {2022-03-25},
	journal = {arXiv:2106.09681 [cs]},
	author = {El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and Jegou, Hervé},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09681},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{li_efficient_2021,
	title = {Efficient {Self}-supervised {Vision} {Transformers} for {Representation} {Learning}},
	url = {http://arxiv.org/abs/2106.09785},
	abstract = {This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task of region matching which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3\% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and models will be publicly available.},
	urldate = {2022-03-25},
	journal = {arXiv:2106.09785 [cs]},
	author = {Li, Chunyuan and Yang, Jianwei and Zhang, Pengchuan and Gao, Mei and Xiao, Bin and Dai, Xiyang and Yuan, Lu and Gao, Jianfeng},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09785},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{zhou_ibot_2022,
	title = {{iBOT}: {Image} {BERT} {Pre}-{Training} with {Online} {Tokenizer}},
	shorttitle = {{iBOT}},
	url = {http://arxiv.org/abs/2111.07832},
	abstract = {The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the advantages and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework iBOT that can perform masked prediction with an online tokenizer. Specifically, we perform self-distillation on masked patch tokens and take the teacher network as the online tokenizer, along with self-distillation on the class token to acquire visual semantics. The online tokenizer is jointly learnable with the MIM objective and dispenses with a multi-stage training pipeline where the tokenizer needs to be pre-trained beforehand. We show the prominence of iBOT by achieving an 82.3\% linear probing accuracy and an 87.8\% fine-tuning accuracy evaluated on ImageNet-1K. Beyond the state-of-the-art image classification results, we underline emerging local semantic patterns, which helps the models to obtain strong robustness against common corruptions and achieve leading results on dense downstream tasks, eg., object detection, instance segmentation, and semantic segmentation.},
	urldate = {2022-03-25},
	journal = {arXiv:2111.07832 [cs]},
	author = {Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
	month = jan,
	year = {2022},
	note = {arXiv: 2111.07832},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jia_scaling_2021,
	title = {Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}},
	url = {http://arxiv.org/abs/2102.05918},
	abstract = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
	urldate = {2022-03-25},
	journal = {arXiv:2102.05918 [cs]},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
	month = jun,
	year = {2021},
	note = {arXiv: 2102.05918},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@article{bao_beit_2021,
	title = {{BEiT}: {BERT} {Pre}-{Training} of {Image} {Transformers}},
	shorttitle = {{BEiT}},
	url = {http://arxiv.org/abs/2106.08254},
	abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
	urldate = {2022-03-25},
	journal = {arXiv:2106.08254 [cs]},
	author = {Bao, Hangbo and Dong, Li and Wei, Furu},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08254},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{singh_revisiting_2022,
	title = {Revisiting {Weakly} {Supervised} {Pre}-{Training} of {Visual} {Perception} {Models}},
	url = {http://arxiv.org/abs/2201.08371},
	abstract = {Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.},
	urldate = {2022-03-25},
	journal = {arXiv:2201.08371 [cs]},
	author = {Singh, Mannat and Gustafson, Laura and Adcock, Aaron and Reis, Vinicius de Freitas and Gedik, Bugra and Kosaraju, Raj Prateek and Mahajan, Dhruv and Girshick, Ross and Dollár, Piotr and van der Maaten, Laurens},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.08371},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shit_cldice_2021,
	title = {{clDice} -- a {Novel} {Topology}-{Preserving} {Loss} {Function} for {Tubular} {Structure} {Segmentation}},
	url = {http://arxiv.org/abs/2003.07311},
	abstract = {Accurate segmentation of tubular, network-like structures, such as vessels, neurons, or roads, is relevant to many fields of research. For such structures, the topology is their most important characteristic; particularly preserving connectedness: in the case of vascular networks, missing a connected vessel entirely alters the blood-flow dynamics. We introduce a novel similarity measure termed centerlineDice (short clDice), which is calculated on the intersection of the segmentation masks and their (morphological) skeleta. We theoretically prove that clDice guarantees topology preservation up to homotopy equivalence for binary 2D and 3D segmentation. Extending this, we propose a computationally efficient, differentiable loss function (soft-clDice) for training arbitrary neural segmentation networks. We benchmark the soft-clDice loss on five public datasets, including vessels, roads and neurons (2D and 3D). Training on soft-clDice leads to segmentation with more accurate connectivity information, higher graph similarity, and better volumetric scores.},
	urldate = {2022-03-04},
	journal = {arXiv:2003.07311 [cs, eess]},
	author = {Shit, Suprosanna and Paetzold, Johannes C. and Sekuboyina, Anjany and Ezhov, Ivan and Unger, Alexander and Zhylka, Andrey and Pluim, Josien P. W. and Bauer, Ulrich and Menze, Bjoern H.},
	month = mar,
	year = {2021},
	note = {arXiv: 2003.07311},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{isensee_nnu-net_2018,
	title = {{nnU}-{Net}: {Self}-adapting {Framework} for {U}-{Net}-{Based} {Medical} {Image} {Segmentation}},
	shorttitle = {{nnU}-{Net}},
	url = {http://arxiv.org/abs/1809.10486},
	abstract = {The U-Net was presented in 2015. With its straight-forward and successful architecture it quickly evolved to a commonly used benchmark in medical image segmentation. The adaptation of the U-Net to novel problems, however, comprises several degrees of freedom regarding the exact architecture, preprocessing, training and inference. These choices are not independent of each other and substantially impact the overall performance. The present paper introduces the nnU-Net ('no-new-Net'), which refers to a robust and self-adapting framework on the basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking away superfluous bells and whistles of many proposed network designs and instead focus on the remaining aspects that make out the performance and generalizability of a method. We evaluate the nnU-Net in the context of the Medical Segmentation Decathlon challenge, which measures segmentation performance in ten disciplines comprising distinct entities, image modalities, image geometries and dataset sizes, with no manual adjustments between datasets allowed. At the time of manuscript submission, nnU-Net achieves the highest mean dice scores across all classes and seven phase 1 tasks (except class 1 in BrainTumour) in the online leaderboard of the challenge.},
	urldate = {2022-03-04},
	journal = {arXiv:1809.10486 [cs]},
	author = {Isensee, Fabian and Petersen, Jens and Klein, Andre and Zimmerer, David and Jaeger, Paul F. and Kohl, Simon and Wasserthal, Jakob and Koehler, Gregor and Norajitra, Tobias and Wirkert, Sebastian and Maier-Hein, Klaus H.},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.10486},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhang_topology-preserving_2022,
	title = {Topology-{Preserving} {Segmentation} {Network}: {A} {Deep} {Learning} {Segmentation} {Framework} for {Connected} {Component}},
	shorttitle = {Topology-{Preserving} {Segmentation} {Network}},
	url = {http://arxiv.org/abs/2202.13331},
	abstract = {Medical image segmentation, which aims to automatically extract anatomical or pathological structures, plays a key role in computer-aided diagnosis and disease analysis. Despite the problem has been widely studied, existing methods are prone to topological errors. In medical imaging, the topology of the structure, such as the kidney or lung, is usually known. Preserving the topology of the structure in the segmentation process is of utmost importance for accurate image analysis. In this work, a novel learning-based segmentation model is proposed. A \{{\textbackslash}it topology-preserving segmentation network (TPSN)\} is trained to give an accurate segmentation result of an input image that preserves the prescribed topology. TPSN is a deformation-based model that yields a deformation map through a UNet, which takes the medical image and a template mask as inputs. The main idea is to deform a template mask describing the prescribed topology by a diffeomorphism to segment the object in the image. The topology of the shape in the template mask is well preserved under the diffeomorphic map. The diffeomorphic property of the map is controlled by introducing a regularization term related to the Jacobian in the loss function. As such, a topology-preserving segmentation result can be guaranteed. Furthermore, a multi-scale TPSN is developed in this paper that incorporates multi-level information of images to produce more precise segmentation results. To evaluate our method, we applied the 2D TPSN on Ham10000 and 3D TPSN on KiTS21. Experimental results illustrate our method outperforms the baseline UNet segmentation model with/without connected-component analysis (CCA) by both the dice score and IoU score. Besides, results show that our method can produce reliable results even in challenging cases, where pixel-wise segmentation models by UNet and CCA fail to obtain accurate results.},
	urldate = {2022-03-04},
	journal = {arXiv:2202.13331 [cs, eess]},
	author = {Zhang, Han and Lui, Lok Ming},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.13331},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{noroozi_unsupervised_2017,
	title = {Unsupervised {Learning} of {Visual} {Representations} by {Solving} {Jigsaw} {Puzzles}},
	url = {http://arxiv.org/abs/1603.09246},
	abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
	urldate = {2022-02-09},
	journal = {arXiv:1603.09246 [cs]},
	author = {Noroozi, Mehdi and Favaro, Paolo},
	month = aug,
	year = {2017},
	note = {arXiv: 1603.09246},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shurrab_self-supervised_2021,
	title = {Self-supervised learning methods and applications in medical imaging analysis: {A} survey},
	shorttitle = {Self-supervised learning methods and applications in medical imaging analysis},
	url = {http://arxiv.org/abs/2109.08685},
	abstract = {The availability of high quality annotated medical imaging datasets is a major problem that collides with machine learning applications in the field of medical imaging analysis and impedes its advancement. Self-supervised learning is a recent training paradigm that enables learning robust representations without the need for human annotation which can be considered as an effective solution for the scarcity in annotated medical data. This article reviews the state-of-the-art research directions in self-supervised learning approaches for image data with concentration on their applications in the field of medical imaging analysis. The article covers a set of the most recent self-supervised learning methods from the computer vision field as they are applicable to the medical imaging analysis and categorize them as predictive, generative and contrastive approaches. Moreover, the article covers (40) of the most recent researches in the field of self-supervised learning in medical imaging analysis aiming at shedding the light on the recent innovation in the field. Ultimately, the article concludes with possible future research directions in the field.},
	urldate = {2022-02-09},
	journal = {arXiv:2109.08685 [cs, eess]},
	author = {Shurrab, Saeed and Duwairi, Rehab},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.08685},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{haghighi_learning_2020,
	title = {Learning {Semantics}-enriched {Representation} via {Self}-discovery, {Self}-classification, and {Self}-restoration},
	url = {http://arxiv.org/abs/2007.06959},
	abstract = {Medical images are naturally associated with rich semantics about the human anatomy, reflected in an abundance of recurring anatomical patterns, offering unique potential to foster deep semantic representation learning and yield semantically more powerful models for different medical applications. But how exactly such strong yet free semantics embedded in medical images can be harnessed for self-supervised learning remains largely unexplored. To this end, we train deep models to learn semantically enriched visual representation by self-discovery, self-classification, and self-restoration of the anatomy underneath medical images, resulting in a semantics-enriched, general-purpose, pre-trained 3D model, named Semantic Genesis. We examine our Semantic Genesis with all the publicly-available pre-trained models, by either self-supervision or fully supervision, on the six distinct target tasks, covering both classification and segmentation in various medical modalities (i.e.,CT, MRI, and X-ray). Our extensive experiments demonstrate that Semantic Genesis significantly exceeds all of its 3D counterparts as well as the de facto ImageNet-based transfer learning in 2D. This performance is attributed to our novel self-supervised learning framework, encouraging deep models to learn compelling semantic representation from abundant anatomical patterns resulting from consistent anatomies embedded in medical images. Code and pre-trained Semantic Genesis are available at https://github.com/JLiangLab/SemanticGenesis .},
	urldate = {2022-02-09},
	journal = {arXiv:2007.06959 [cs, eess]},
	author = {Haghighi, Fatemeh and Taher, Mohammad Reza Hosseinzadeh and Zhou, Zongwei and Gotway, Michael B. and Liang, Jianming},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.06959},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2021-12-15},
	journal = {arXiv:2104.14294 [cs]},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv: 2104.14294
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{mehta_mobilevit_2021,
	title = {{MobileViT}: {Light}-weight, {General}-purpose, and {Mobile}-friendly {Vision} {Transformer}},
	shorttitle = {{MobileViT}},
	url = {http://arxiv.org/abs/2110.02178},
	abstract = {Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4\% with about 6 million parameters, which is 3.2\% and 6.2\% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7\% more accurate than Mo-bileNetv3 for a similar number of parameters.},
	urldate = {2021-12-15},
	journal = {arXiv:2110.02178 [cs]},
	author = {Mehta, Sachin and Rastegari, Mohammad},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.02178},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{dai_coatnet_2021,
	title = {{CoAtNet}: {Marrying} {Convolution} and {Attention} for {All} {Data} {Sizes}},
	shorttitle = {{CoAtNet}},
	url = {http://arxiv.org/abs/2106.04803},
	abstract = {Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced "coat" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0\% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56\% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88\% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.},
	urldate = {2021-12-15},
	journal = {arXiv:2106.04803 [cs]},
	author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V. and Tan, Mingxing},
	month = sep,
	year = {2021},
	note = {arXiv: 2106.04803},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2021-12-15},
	journal = {arXiv:2103.14030 [cs]},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv: 2103.14030},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{tan_efficientnetv2_2021,
	title = {{EfficientNetV2}: {Smaller} {Models} and {Faster} {Training}},
	shorttitle = {{EfficientNetV2}},
	url = {http://arxiv.org/abs/2104.00298},
	abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
	urldate = {2021-12-15},
	journal = {arXiv:2104.00298 [cs]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.00298},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2021-12-15},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@inproceedings{giorgi_automatic_2013,
	title = {Automatic chord recognition based on the probabilistic modeling of diatonic modal harmony},
	abstract = {Chords and keys are among the most exhaustive descriptors of songs. In this study we focus on chord and key sequence recognition from an audio signal, in the context of pop and rock music. The system exploits a set of novel probabilistic models that describe the relationship between different aspects of music and their temporal evolution. These models are based on a set of parameters with a musical meaning. The models include two diatonic key modes, Dorian and Mixolydian, besides major and minor modes previously considered in the literature. These four key modes are the most used in western pop and rock music. In order to provide a compact representation of the chord and key sequences, three novel time-varying harmonybased features are here introduced. Given the importance of emotion characterization in music, the three features are here related to the mood perceived in songs. The method outperforms the state-of-the-art in both chord and key recognition tasks. In order to better train our parameters, we create annotations of chords and keys for a new dataset of 62 songs from the first five Robbie Williams' albums.},
	booktitle = {{nDS} '13; {Proceedings} of the 8th {International} {Workshop} on {Multidimensional} {Systems}},
	author = {Giorgi, Bruno Di and Zanoni, Massimiliano and Sarti, Augusto and Tubaro, Stefano},
	month = sep,
	year = {2013},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/jan/Zotero/storage/D3IQQAJ3/6623838.html:text/html;IEEE Xplore Full Text PDF:/home/jan/Zotero/storage/SYFDSMQA/Giorgi et al. - 2013 - Automatic chord recognition based on the probabili.pdf:application/pdf},
}

@misc{baade_mae-ast_2022,
	title = {{MAE}-{AST}: {Masked} {Autoencoding} {Audio} {Spectrogram} {Transformer}},
	shorttitle = {{MAE}-{AST}},
	url = {http://arxiv.org/abs/2203.16691},
	abstract = {In this paper, we propose a simple yet powerful improvement over the recent Self-Supervised Audio Spectrogram Transformer (SSAST) model for speech and audio classiﬁcation. Speciﬁcally, we leverage the insight that the SSAST uses a very high masking ratio (75\%) during pretraining, meaning that the vast majority of self-attention compute is performed on mask tokens. We address this by integrating the encoder-decoder architecture from Masked Autoencoders are Scalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on only unmasked input, and a shallow decoder operates on encoder outputs and mask tokens. We ﬁnd that MAE-like pretraining can provide a 3× speedup and 2× memory usage reduction over the vanilla SSAST using current audio pretraining strategies with ordinary model and input sizes. When ﬁnetuning on downstream tasks, which only uses the encoder, we ﬁnd that our approach outperforms the SSAST on a variety of downstream tasks. We further conduct comprehensive evaluations into different strategies of pretraining and explore differences in MAE-style pretraining between the visual and audio domains.},
	language = {en},
	urldate = {2022-09-19},
	publisher = {arXiv},
	author = {Baade, Alan and Peng, Puyuan and Harwath, David},
	month = mar,
	year = {2022},
	note = {arXiv:2203.16691 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language},
	annote = {Comment: Submitted to INTERSPEECH. 5 pages, 2 figures, 5 tables},
	file = {Baade et al. - 2022 - MAE-AST Masked Autoencoding Audio Spectrogram Tra.pdf:/home/jan/Zotero/storage/95RMVCKF/Baade et al. - 2022 - MAE-AST Masked Autoencoding Audio Spectrogram Tra.pdf:application/pdf},
}

@misc{chen_exploring_2020,
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2011.10566},
	abstract = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Chen, Xinlei and He, Kaiming},
	month = nov,
	year = {2020},
	note = {arXiv:2011.10566 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Technical report, 10 pages},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/IWHJA9WJ/Chen i He - 2020 - Exploring Simple Siamese Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/XYLZUKSP/2011.html:text/html},
}

@misc{hassani_dilated_2022,
	title = {Dilated {Neighborhood} {Attention} {Transformer}},
	url = {http://arxiv.org/abs/2209.15001},
	abstract = {Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over attention-based baselines such as NAT and Swin, as well as modern convolutional baseline ConvNeXt. Our Large model is ahead of its Swin counterpart by 1.5\% box AP in COCO object detection, 1.3\% mask AP in COCO instance segmentation, and 1.1\% mIoU in ADE20K semantic segmentation, and faster in throughput. We believe combinations of NA and DiNA have the potential to empower various tasks beyond those presented in this paper. To support and encourage research in this direction, in vision and beyond, we open-source our project at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Hassani, Ali and Shi, Humphrey},
	month = sep,
	year = {2022},
	note = {arXiv:2209.15001 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/33J5R3CE/Hassani i Shi - 2022 - Dilated Neighborhood Attention Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/WSCLT46G/2209.html:text/html},
}

@misc{li_understanding_2022,
	title = {Understanding {Collapse} in {Non}-{Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2209.15007},
	abstract = {Contrastive methods have led a recent surge in the performance of self-supervised representation learning (SSL). Recent methods like BYOL or SimSiam purportedly distill these contrastive methods down to their essence, removing bells and whistles, including the negative examples, that do not contribute to downstream performance. These "non-contrastive" methods work surprisingly well without using negatives even though the global minimum lies at trivial collapse. We empirically analyze these non-contrastive methods and find that SimSiam is extraordinarily sensitive to dataset and model size. In particular, SimSiam representations undergo partial dimensional collapse if the model is too small relative to the dataset size. We propose a metric to measure the degree of this collapse and show that it can be used to forecast the downstream task performance without any fine-tuning or labels. We further analyze architectural design choices and their effect on the downstream performance. Finally, we demonstrate that shifting to a continual learning setting acts as a regularizer and prevents collapse, and a hybrid between continual and multi-epoch training can improve linear probe accuracy by as many as 18 percentage points using ResNet-18 on ImageNet.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Li, Alexander C. and Efros, Alexei A. and Pathak, Deepak},
	month = sep,
	year = {2022},
	note = {arXiv:2209.15007 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	annote = {Comment: Published at ECCV 2022},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/9PYUTSEY/Li et al. - 2022 - Understanding Collapse in Non-Contrastive Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/8WVC878B/2209.html:text/html},
}

@misc{bardes_vicregl_2022,
	title = {{VICRegL}: {Self}-{Supervised} {Learning} of {Local} {Visual} {Features}},
	shorttitle = {{VICRegL}},
	url = {http://arxiv.org/abs/2210.01571},
	abstract = {Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL},
	language = {en},
	urldate = {2022-10-08},
	publisher = {arXiv},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01571 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at NeurIPS 2022},
	file = {Bardes et al. - 2022 - VICRegL Self-Supervised Learning of Local Visual .pdf:/home/jan/Zotero/storage/UXSKD5JZ/Bardes et al. - 2022 - VICRegL Self-Supervised Learning of Local Visual .pdf:application/pdf},
}

@misc{bardes_vicreg_2022,
	title = {{VICReg}: {Variance}-{Invariance}-{Covariance} {Regularization} for {Self}-{Supervised} {Learning}},
	shorttitle = {{VICReg}},
	url = {http://arxiv.org/abs/2105.04906},
	abstract = {Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with two regularizations terms applied to both embeddings separately: (1) a term that maintains the variance of each embedding dimension above a threshold, (2) a term that decorrelates each pair of variables. Unlike most other approaches to the same problem, VICReg does not require techniques such as: weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state of the art on several downstream tasks. In addition, we show that our variance regularization term stabilizes the training of other methods and leads to performance improvements.},
	language = {en},
	urldate = {2022-10-08},
	publisher = {arXiv},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	month = jan,
	year = {2022},
	note = {arXiv:2105.04906 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at ICLR 2022},
	file = {Bardes et al. - 2022 - VICReg Variance-Invariance-Covariance Regularizat.pdf:/home/jan/Zotero/storage/6J69EHG6/Bardes et al. - 2022 - VICReg Variance-Invariance-Covariance Regularizat.pdf:application/pdf},
}

@misc{cubuk_randaugment_2019,
	title = {{RandAugment}: {Practical} automated data augmentation with a reduced search space},
	shorttitle = {{RandAugment}},
	url = {http://arxiv.org/abs/1909.13719},
	abstract = {Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0\% accuracy, a 0.6\% increase over the previous state-of-the-art and 1.0\% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3\% improvement over baseline augmentation, and is within 0.3\% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
	month = nov,
	year = {2019},
	note = {arXiv:1909.13719 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	language = {en},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hu_exploring_2022,
	title = {Exploring {Long}-{Sequence} {Masked} {Autoencoders}},
	url = {http://arxiv.org/abs/2210.07224},
	abstract = {Masked Autoencoding (MAE) has emerged as an effective approach for pre-training representations across multiple domains. In contrast to discrete tokens in natural languages, the input for image MAE is continuous and subject to additional speciﬁcations. We systematically study each input speciﬁcation during the pre-training stage, and ﬁnd sequence length is a key axis that further scales MAE. Our study leads to a long-sequence version of MAE with minimal changes to the original recipe, by just decoupling the mask size from the patch size. For object detection and semantic segmentation, our long-sequence MAE shows consistent gains across all the experimental setups without extra computation cost during the transfer. While long-sequence pre-training is discerned most beneﬁcial for detection and segmentation, we also achieve strong results on ImageNet-1K classiﬁcation by keeping a standard image size and only increasing the sequence length. We hope our ﬁndings can provide new insights and avenues for scaling in computer vision.},
	language = {en},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Hu, Ronghang and Debnath, Shoubhik and Xie, Saining and Chen, Xinlei},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07224 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 11 pages},
	file = {Hu et al. - 2022 - Exploring Long-Sequence Masked Autoencoders.pdf:/home/jan/Zotero/storage/NXKMLD5M/Hu et al. - 2022 - Exploring Long-Sequence Masked Autoencoders.pdf:application/pdf},
}

@misc{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1502.03167 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we ﬁnd that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efﬁciently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.},
	language = {en},
	urldate = {2022-10-31},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report. arXiv v2: add more transfer learning results; v3: add robustness evaluation},
	file = {He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:/home/jan/Zotero/storage/ABEU9XWA/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf},
}

@misc{li_exploring_2022,
	title = {Exploring {Plain} {Vision} {Transformer} {Backbones} for {Object} {Detection}},
	url = {http://arxiv.org/abs/2203.16527},
	doi = {10.48550/arXiv.2203.16527},
	abstract = {We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP\_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
	month = jun,
	year = {2022},
	note = {arXiv:2203.16527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{hatamizadeh_unetformer_2022,
	title = {{UNetFormer}: {A} {Unified} {Vision} {Transformer} {Model} and {Pre}-{Training} {Framework} for {3D} {Medical} {Image} {Segmentation}},
	shorttitle = {{UNetFormer}},
	url = {http://arxiv.org/abs/2204.00631},
	doi = {10.48550/arXiv.2204.00631},
	abstract = {Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of \$5050\$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Hatamizadeh, Ali and Xu, Ziyue and Yang, Dong and Li, Wenqi and Roth, Holger and Xu, Daguang},
	month = apr,
	year = {2022},
	note = {arXiv:2204.00631 [cs, eess]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Artificial Intelligence},
}

@misc{zhou_nnformer_2022,
	title = {{nnFormer}: {Interleaved} {Transformer} for {Volumetric} {Segmentation}},
	shorttitle = {{nnFormer}},
	url = {http://arxiv.org/abs/2109.03201},
	doi = {10.48550/arXiv.2109.03201},
	abstract = {Transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to overcome their inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. To address this issue, we introduce nnFormer, a 3D transformer for volumetric medical image segmentation. nnFormer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. Moreover, nnFormer proposes to use skip attention to replace the traditional concatenation/summation operations in skip connections in U-Net like architecture. Experiments show that nnFormer significantly outperforms previous transformer-based counterparts by large margins on three public datasets. Compared to nnUNet, nnFormer produces significantly lower HD95 and comparable DSC results. Furthermore, we show that nnFormer and nnUNet are highly complementary to each other in model ensembling.},
	urldate = {2022-11-02},
	publisher = {arXiv},
	author = {Zhou, Hong-Yu and Guo, Jiansen and Zhang, Yinghao and Yu, Lequan and Wang, Liansheng and Yu, Yizhou},
	month = feb,
	year = {2022},
	note = {arXiv:2109.03201 [cs]
version: 6},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ranftl_vision_2021,
	title = {Vision {Transformers} for {Dense} {Prediction}},
	url = {http://arxiv.org/abs/2103.13413},
	abstract = {We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28\% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02\% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Ranftl, René and Bochkovskiy, Alexey and Koltun, Vladlen},
	month = mar,
	year = {2021},
	note = {arXiv:2103.13413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/PI6YI76H/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/2U3NBFCY/2006.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition, Condensed Matter - Disordered Systems and Neural Networks},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/YS269DYS/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/IHR87QEG/1503.html:text/html},
}

@misc{woo_convnext_2023,
	title = {{ConvNeXt} {V2}: {Co}-designing and {Scaling} {ConvNets} with {Masked} {Autoencoders}},
	shorttitle = {{ConvNeXt} {V2}},
	url = {http://arxiv.org/abs/2301.00808},
	abstract = {Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7\% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9\% accuracy using only public training data.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00808 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code and models available at https://github.com/facebookresearch/ConvNeXt-V2},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/IIP3RPML/Woo et al. - 2023 - ConvNeXt V2 Co-designing and Scaling ConvNets wit.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/D7K4B7FC/2301.html:text/html},
}

@inproceedings{mcfee_librosa_2015,
	address = {Austin, Texas},
	title = {librosa: {Audio} and {Music} {Signal} {Analysis} in {Python}},
	shorttitle = {librosa},
	url = {https://conference.scipy.org/proceedings/scipy2015/brian_mcfee.html},
	doi = {10.25080/Majora-7b98e3ed-003},
	abstract = {This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the ﬁeld of music information retrieval. In this document, a brief overview of the library’s functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
	language = {en},
	urldate = {2023-01-14},
	author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
	year = {2015},
	pages = {18--24},
	file = {McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf:/home/jan/Zotero/storage/A4CUKI5K/McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf:application/pdf},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2023-01-19},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/5NT8AXUF/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/9NMHKRK3/2103.html:text/html},
}

@misc{pathak_context_2016,
	title = {Context {Encoders}: {Feature} {Learning} by {Inpainting}},
	shorttitle = {Context {Encoders}},
	url = {http://arxiv.org/abs/1604.07379},
	abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
	month = nov,
	year = {2016},
	note = {arXiv:1604.07379 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	annote = {Comment: New results on ImageNet Generation},
	file = {arXiv Fulltext PDF:/home/jan/Zotero/storage/4QP65EVP/Pathak et al. - 2016 - Context Encoders Feature Learning by Inpainting.pdf:application/pdf;arXiv.org Snapshot:/home/jan/Zotero/storage/6K4EBEPJ/1604.html:text/html},
}

@book{lerch_introduction_2012,
	address = {Hoboken, New Jersey},
	title = {An {Introduction} to {Audio} {Content} {Analysis}: {Applications} in {Signal} {Processing} and {Music} {Informatics}},
	isbn = {978-1-118-26682-3},
	publisher = {John Wiley \& Sons},
	author = {Lerch, Alexander},
	year = {2012},
}

@book{lyons_wprowadzenie_2000,
	address = {Warszawa},
	edition = {1},
	title = {Wprowadzenie do cyfrowego przetwarzania sygnałów},
	isbn = {83-206-1318-3},
	publisher = {Wydawnictwo Komunikacji i Łączności},
	author = {Lyons, Richard G.},
	year = {2000},
}

@inproceedings{fujishima_realtime_1999,
	title = {Realtime {Chord} {Recognition} of {Musical} {Sound}: a {System} {Using} {Common} {Lisp} {Music}},
	booktitle = {International {Conference} on {Mathematics} and {Computing}},
	author = {Fujishima, Takuya},
	year = {1999},
}
